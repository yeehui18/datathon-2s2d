{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "N_smoqVgjUOW",
   "metadata": {
    "id": "N_smoqVgjUOW"
   },
   "source": [
    "# **NUS DATHATON 2026**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cwbDfwIyjhXv",
   "metadata": {
    "id": "cwbDfwIyjhXv"
   },
   "source": [
    "***2 Sons 2 Daughters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2FFsgBJyi4r0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FFsgBJyi4r0",
    "outputId": "fd2b083d-d7e1-4c15-d4ac-3882eed31e39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping numpy as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aCophhuEhVCg",
   "metadata": {
    "id": "aCophhuEhVCg",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mticker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultipleLocator, AutoMinorLocator\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpatches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpatches\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_ncp7c0BntKf",
   "metadata": {
    "id": "_ncp7c0BntKf"
   },
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SAlT9GR6ny6M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "SAlT9GR6ny6M",
    "outputId": "b8624c6b-3f1a-456e-e061-a71a9874eb5c"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload(\"champions_group_data.xlsx\")\n",
    "fname = next(iter(uploaded))   # gets the uploaded filename\n",
    "df_raw = pd.read_excel(fname)\n",
    "print(\"Loaded:\", fname, \"shape:\", df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mbAHH2C-DE5R",
   "metadata": {
    "id": "mbAHH2C-DE5R"
   },
   "source": [
    "#clean data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wTAolpVjDDyN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "id": "wTAolpVjDDyN",
    "outputId": "59cb29b4-6e91-4277-c892-797b28c64893"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _normalise_colname(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[%()\\/]\", \" \", s)\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def _clean_string_series(x: pd.Series) -> pd.Series:\n",
    "    x = x.astype(\"string\")\n",
    "    x = x.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    x = x.replace(\n",
    "        {\n",
    "            \"\": pd.NA,\n",
    "            \"na\": pd.NA, \"n/a\": pd.NA, \"none\": pd.NA, \"null\": pd.NA,\n",
    "            \"unknown\": pd.NA, \"not available\": pd.NA, \"not applicable\": pd.NA,\n",
    "        }\n",
    "    )\n",
    "    return x\n",
    "\n",
    "def _to_float(x: pd.Series) -> pd.Series:\n",
    "    if x.dtype.kind in \"if\":\n",
    "        return x.astype(\"float\")\n",
    "    x = _clean_string_series(x)\n",
    "    # remove currency symbols and commas, keep digits, dot, minus\n",
    "    x = x.str.replace(r\"[$£€,\\s]\", \"\", regex=True)\n",
    "    # handle parentheses negatives like (123)\n",
    "    x = x.str.replace(r\"^\\((.+)\\)$\", r\"-\\1\", regex=True)\n",
    "    # keep only valid number-like\n",
    "    x = x.where(x.str.match(r\"^-?\\d+(\\.\\d+)?$\", na=False), pd.NA)\n",
    "    return x.astype(\"float\")\n",
    "\n",
    "def _bucket_midpoint(val: str):\n",
    "    \"\"\"\n",
    "    Handles:\n",
    "      '1 to 10'\n",
    "      '11 - 50'\n",
    "      '1,001 to 5,000'\n",
    "      '100000+' or '100,000+'\n",
    "    Returns midpoint as float or NaN.\n",
    "    \"\"\"\n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "        return np.nan\n",
    "    s = str(val).strip().lower()\n",
    "    if s in {\"\", \"na\", \"n/a\", \"none\", \"null\", \"unknown\"}:\n",
    "        return np.nan\n",
    "\n",
    "    s = s.replace(\",\", \"\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "\n",
    "    # plus buckets\n",
    "    m = re.match(r\"^(\\d+)\\s*\\+?$\", s)\n",
    "    if m:\n",
    "        return float(m.group(1))\n",
    "\n",
    "    # ranges with 'to' or '-'\n",
    "    m = re.match(r\"^(\\d+)\\s*(to|-)\\s*(\\d+)$\", s)\n",
    "    if m:\n",
    "        a = float(m.group(1))\n",
    "        b = float(m.group(3))\n",
    "        return (a + b) / 2.0\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "def clean_company_data(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # 1) normalise column names\n",
    "    df.columns = [_normalise_colname(c) for c in df.columns]\n",
    "\n",
    "    # 2) standardise obvious string columns\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\":\n",
    "            df[c] = _clean_string_series(df[c])\n",
    "\n",
    "    # 3) ensure key IDs are strings if present\n",
    "    for c in [\"company_id\", \"id\", \"duns\", \"sic_code\", \"naics_code\", \"nace_code\", \"isic_code\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = _clean_string_series(df[c]).astype(\"string\")\n",
    "\n",
    "    # 4) numeric conversions where expected\n",
    "    numeric_candidates = [\n",
    "        \"employees_total\",\n",
    "        \"revenue_usd\",\n",
    "        \"year_found\",\n",
    "        \"corporate_family_members\",\n",
    "        \"it_budget\",\n",
    "        \"it_spend\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "    ]\n",
    "    for c in numeric_candidates:\n",
    "        if c in df.columns:\n",
    "            df[c] = _to_float(df[c])\n",
    "\n",
    "    # 5) bucket to midpoint conversion for device footprint columns\n",
    "    bucket_cols = [\n",
    "        \"no_of_pc\",\n",
    "        \"no_of_desktops\",\n",
    "        \"no_of_laptops\",\n",
    "        \"no_of_routers\",\n",
    "        \"no_of_servers\",\n",
    "        \"no_of_storage_devices\",\n",
    "    ]\n",
    "    for c in bucket_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].map(_bucket_midpoint).astype(\"float\")\n",
    "\n",
    "    # 6) basic sanity fixes\n",
    "    if \"year_found\" in df.columns:\n",
    "        df.loc[(df[\"year_found\"] < 1700) | (df[\"year_found\"] > 2030), \"year_found\"] = np.nan\n",
    "\n",
    "    if \"employees_total\" in df.columns:\n",
    "        df.loc[df[\"employees_total\"] < 0, \"employees_total\"] = np.nan\n",
    "\n",
    "    if \"revenue_usd\" in df.columns:\n",
    "        df.loc[df[\"revenue_usd\"] < 0, \"revenue_usd\"] = np.nan\n",
    "\n",
    "    # 7) derived features that help segmentation\n",
    "    if \"revenue_usd\" in df.columns and \"employees_total\" in df.columns:\n",
    "        df[\"revenue_per_employee\"] = df[\"revenue_usd\"] / df[\"employees_total\"]\n",
    "        df.loc[~np.isfinite(df[\"revenue_per_employee\"]), \"revenue_per_employee\"] = np.nan\n",
    "\n",
    "    if \"it_spend\" in df.columns and \"revenue_usd\" in df.columns:\n",
    "        df[\"it_spend_to_revenue\"] = df[\"it_spend\"] / df[\"revenue_usd\"]\n",
    "        df.loc[~np.isfinite(df[\"it_spend_to_revenue\"]), \"it_spend_to_revenue\"] = np.nan\n",
    "\n",
    "    # 8) drop columns that are basically empty (tune threshold if needed)\n",
    "    missing_rate = df.isna().mean()\n",
    "    mostly_empty = missing_rate[missing_rate >= 0.99].index.tolist()\n",
    "    df = df.drop(columns=mostly_empty)\n",
    "\n",
    "    # 9) de duplicate rows if an obvious unique key exists\n",
    "    for key in [\"company_id\", \"id\", \"duns\", \"company_name\"]:\n",
    "        if key in df.columns:\n",
    "            df = df.drop_duplicates(subset=[key], keep=\"first\")\n",
    "            break\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_clean = clean_company_data(df_raw)\n",
    "print(df_raw.shape, \"->\", df_clean.shape)\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OzuIxWdbJc_8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "OzuIxWdbJc_8",
    "outputId": "b32ec257-a4b9-4783-8b93-79ff4c42d0d7"
   },
   "outputs": [],
   "source": [
    "# === Step 1: Build the clustering feature table (X) ===\n",
    "# Assumes you already have: df_clean (your cleaned dataframe)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = df_clean.copy()\n",
    "\n",
    "# --- Core columns for segmentation (v1) ---\n",
    "industry_cols = [\"sic_code\", \"sic_description\", \"8_digit_sic_code\", \"8_digit_sic_description\"]\n",
    "\n",
    "size_cols = [\"employees_total\", \"revenue_usd\", \"revenue_per_employee\"]\n",
    "\n",
    "structure_cols = [\"entity_type\", \"corporate_family_members\", \"is_headquarters\", \"is_domestic_ultimate\"]\n",
    "\n",
    "it_cols = [\n",
    "    \"it_budget\", \"it_spend\",\n",
    "    \"no_of_pc\", \"no_of_desktops\", \"no_of_laptops\",\n",
    "    \"no_of_routers\", \"no_of_servers\", \"no_of_storage_devices\",\n",
    "    \"it_spend_to_revenue\"\n",
    "]\n",
    "\n",
    "geo_cols = [\"country\", \"region\"]  # keep it coarse to avoid exploding categories\n",
    "\n",
    "# Prefer 8-digit SIC if available; else regular SIC; else fall back to SIC description\n",
    "# (this avoids redundant, high-cardinality columns)\n",
    "preferred_industry = []\n",
    "if \"8_digit_sic_code\" in df.columns:\n",
    "    preferred_industry.append(\"8_digit_sic_code\")\n",
    "elif \"sic_code\" in df.columns:\n",
    "    preferred_industry.append(\"sic_code\")\n",
    "\n",
    "# Add a description field if it exists (optional but useful)\n",
    "if \"8_digit_sic_description\" in df.columns:\n",
    "    preferred_industry.append(\"8_digit_sic_description\")\n",
    "elif \"sic_description\" in df.columns:\n",
    "    preferred_industry.append(\"sic_description\")\n",
    "\n",
    "core_cols = preferred_industry + size_cols + structure_cols + it_cols + geo_cols\n",
    "core_cols = [c for c in core_cols if c in df.columns]  # keep only existing\n",
    "\n",
    "# --- Derived hierarchy flags (avoid clustering on raw company name strings) ---\n",
    "# These are boolean features summarising parent/ultimate presence.\n",
    "name_cols_for_flags = [\"parent_company\", \"global_ultimate_company\", \"domestic_ultimate_company\"]\n",
    "for c in name_cols_for_flags:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(\"string\").str.strip()\n",
    "\n",
    "if \"parent_company\" in df.columns:\n",
    "    df[\"has_parent_company\"] = df[\"parent_company\"].notna()\n",
    "else:\n",
    "    df[\"has_parent_company\"] = False\n",
    "\n",
    "if \"global_ultimate_company\" in df.columns:\n",
    "    df[\"has_global_ultimate\"] = df[\"global_ultimate_company\"].notna()\n",
    "else:\n",
    "    df[\"has_global_ultimate\"] = False\n",
    "\n",
    "if \"domestic_ultimate_company\" in df.columns:\n",
    "    df[\"has_domestic_ultimate_company\"] = df[\"domestic_ultimate_company\"].notna()\n",
    "else:\n",
    "    df[\"has_domestic_ultimate_company\"] = False\n",
    "\n",
    "derived_flag_cols = [\"has_parent_company\", \"has_global_ultimate\", \"has_domestic_ultimate_company\"]\n",
    "\n",
    "# --- Build X ---\n",
    "X = df[core_cols + derived_flag_cols].copy()\n",
    "\n",
    "# --- Quick sanity check summary ---\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"\\nColumns used:\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "missing_pct = (X.isna().mean().sort_values(ascending=False) * 100).round(1)\n",
    "print(\"\\nTop 15 columns by missing %:\")\n",
    "print(missing_pct.head(15))\n",
    "\n",
    "# Optional: view a few rows\n",
    "X.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qIyCNKGOMga5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "qIyCNKGOMga5",
    "outputId": "b7f08eea-e9d1-4203-e13f-3a903a7468d6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X2 = X.copy()\n",
    "\n",
    "# Ensure numeric columns are truly numeric (float) and replace pd.NA with np.nan\n",
    "for col in num_cols:\n",
    "    # pd.to_numeric handles various forms of missing data (like pd.NA, or strings like 'N/A')\n",
    "    # and converts them to np.nan upon conversion to float. 'coerce' turns unconvertible values to NaN.\n",
    "    X2[col] = pd.to_numeric(X2[col], errors='coerce')\n",
    "\n",
    "# Ensure categorical columns are 'object' dtype and replace any pd.NA with np.nan\n",
    "for col in cat_cols:\n",
    "    # Convert pandas 'string' dtype to Python 'object' dtype if present\n",
    "    if pd.api.types.is_string_dtype(X2[col]) or X2[col].dtype.name == \"string\":\n",
    "        X2[col] = X2[col].astype(\"object\")\n",
    "    # Replace any pd.NA that might still exist in object columns with np.nan\n",
    "    X2[col] = X2[col].replace({pd.NA: np.nan})\n",
    "\n",
    "# A final global replacement for any pd.NA that might have been missed, just to be safe.\n",
    "# This ensures that before passing to sklearn, no pd.NA values remain anywhere.\n",
    "X2 = X2.replace({pd.NA: np.nan})\n",
    "\n",
    "X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zlaf5jLkLza5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlaf5jLkLza5",
    "outputId": "5898bb60-5c02-4790-fe45-1df1560e9584"
   },
   "outputs": [],
   "source": [
    "# Identify numeric vs categorical columns inside X\n",
    "num_cols = X.select_dtypes(include=[\"number\", \"bool\"]).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "print(\"Numeric cols:\", num_cols)\n",
    "print(\"Categorical cols:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40932005",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40932005",
    "outputId": "9fc08563-9075-4944-9fba-72e46ea39ded"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", numeric_pipe, num_cols),\n",
    "    (\"cat\", categorical_pipe, cat_cols),\n",
    "])\n",
    "\n",
    "X_mat = preprocess.fit_transform(X2)\n",
    "print(\"Transformed shape:\", X_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bRVBxZXRNUN-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRVBxZXRNUN-",
    "outputId": "eedd99a0-b4a3-44a1-ca2d-0976cdaec420"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "ks = range(3, 11)\n",
    "scores = []\n",
    "\n",
    "for k in ks:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=\"auto\")\n",
    "    labels = km.fit_predict(X_mat)\n",
    "    s = silhouette_score(X_mat, labels)\n",
    "    scores.append(s)\n",
    "    print(f\"k={k}  silhouette={s:.4f}\")\n",
    "\n",
    "best_k = list(ks)[int(max(range(len(scores)), key=lambda i: scores[i]))]\n",
    "print(\"Best k:\", best_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3uASMubcNmtf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "3uASMubcNmtf",
    "outputId": "375e8204-6625-4f90-b1cb-6f39b48fd6c3"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=\"auto\")\n",
    "segments = kmeans.fit_predict(X_mat)\n",
    "\n",
    "df_segmented = df.copy()\n",
    "df_segmented[\"segment\"] = segments\n",
    "\n",
    "df_segmented[\"segment\"].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mo7dmR1-QZKG",
   "metadata": {
    "id": "mo7dmR1-QZKG"
   },
   "source": [
    "#new clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VyCOzdBuQeCT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 640
    },
    "id": "VyCOzdBuQeCT",
    "outputId": "f220d03e-661f-4353-8239-ea4e032919e9"
   },
   "outputs": [],
   "source": [
    "# === PLAN A: Rule-based segmentation (Industry + Size + Structure + IT + Geo) ===\n",
    "# Assumes you already have df_clean\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = df_clean.copy()\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _digits_only(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r\"\\D+\", \"\", s)  # keep digits only\n",
    "    return s if s != \"\" else np.nan\n",
    "\n",
    "def sic_prefix(x, n=2):\n",
    "    s = _digits_only(x)\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    if len(s) >= n:\n",
    "        return s[:n]\n",
    "    return s.zfill(n)\n",
    "\n",
    "def safe_qcut(series, q=4, labels=None):\n",
    "    \"\"\"Quantile binning that won't crash if not enough unique values.\"\"\"\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    if s.notna().sum() < q * 5:\n",
    "        # too few points -> fallback to 3 bins\n",
    "        q = 3\n",
    "        labels = labels[:3] if labels is not None else None\n",
    "    try:\n",
    "        return pd.qcut(s, q=q, labels=labels, duplicates=\"drop\")\n",
    "    except Exception:\n",
    "        # fallback: all Unknown\n",
    "        return pd.Series(pd.NA, index=series.index, dtype=\"string\")\n",
    "\n",
    "# ---------- 1) Industry bucket ----------\n",
    "sic_col = \"8_digit_sic_code\" if \"8_digit_sic_code\" in df.columns else \"sic_code\"\n",
    "df[\"sic_2digit\"] = df[sic_col].map(lambda x: sic_prefix(x, n=2))\n",
    "\n",
    "# ---------- 2) Size tiers (employees + revenue) ----------\n",
    "for c in [\"employees_total\", \"revenue_usd\", \"it_spend\", \"it_budget\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df[\"log_employees\"] = np.log1p(df[\"employees_total\"])\n",
    "df[\"log_revenue\"] = np.log1p(df[\"revenue_usd\"])\n",
    "\n",
    "df[\"size_emp_tier\"] = safe_qcut(\n",
    "    df[\"log_employees\"], q=4, labels=[\"emp_s\", \"emp_m\", \"emp_l\", \"emp_xl\"]\n",
    ").astype(\"string\")\n",
    "\n",
    "df[\"size_rev_tier\"] = safe_qcut(\n",
    "    df[\"log_revenue\"], q=4, labels=[\"rev_s\", \"rev_m\", \"rev_l\", \"rev_xl\"]\n",
    ").astype(\"string\")\n",
    "\n",
    "# ---------- 3) Corporate structure tier ----------\n",
    "# Make sure boolean-like columns behave\n",
    "for b in [\"is_headquarters\", \"is_domestic_ultimate\"]:\n",
    "    if b in df.columns:\n",
    "        # convert common string booleans to True/False\n",
    "        if df[b].dtype.name in [\"string\", \"object\"]:\n",
    "            df[b] = df[b].astype(\"string\").str.lower().map({\"true\": True, \"false\": False})\n",
    "        df[b] = df[b].fillna(False).astype(bool)\n",
    "    else:\n",
    "        df[b] = False\n",
    "\n",
    "# derived presence flags\n",
    "if \"parent_company\" in df.columns:\n",
    "    df[\"has_parent_company\"] = df[\"parent_company\"].notna()\n",
    "else:\n",
    "    df[\"has_parent_company\"] = False\n",
    "\n",
    "if \"global_ultimate_company\" in df.columns:\n",
    "    df[\"has_global_ultimate\"] = df[\"global_ultimate_company\"].notna()\n",
    "else:\n",
    "    df[\"has_global_ultimate\"] = False\n",
    "\n",
    "if \"domestic_ultimate_company\" in df.columns:\n",
    "    df[\"has_domestic_ultimate_company\"] = df[\"domestic_ultimate_company\"].notna()\n",
    "else:\n",
    "    df[\"has_domestic_ultimate_company\"] = False\n",
    "\n",
    "def structure_tier(row):\n",
    "    if row.get(\"is_headquarters\", False):\n",
    "        return \"hq\"\n",
    "    if row.get(\"is_domestic_ultimate\", False):\n",
    "        return \"domestic_ultimate\"\n",
    "    # If entity_type exists, use it as first signal\n",
    "    et = str(row.get(\"entity_type\", \"\")).lower()\n",
    "    if \"subsidi\" in et:\n",
    "        return \"subsidiary\"\n",
    "    if \"branch\" in et:\n",
    "        return \"branch\"\n",
    "    if row.get(\"has_parent_company\", False):\n",
    "        return \"subsidiary_like\"\n",
    "    if row.get(\"has_global_ultimate\", False) or row.get(\"has_domestic_ultimate_company\", False):\n",
    "        return \"member_of_group\"\n",
    "    return \"standalone_like\"\n",
    "\n",
    "df[\"structure_tier\"] = df.apply(structure_tier, axis=1).astype(\"string\")\n",
    "\n",
    "# ---------- 4) IT footprint tiers (spend + device footprint) ----------\n",
    "if \"it_spend\" in df.columns:\n",
    "    df[\"log_it_spend\"] = np.log1p(df[\"it_spend\"])\n",
    "    df[\"it_spend_tier\"] = safe_qcut(\n",
    "        df[\"log_it_spend\"], q=4, labels=[\"it_low\", \"it_mid\", \"it_high\", \"it_top\"]\n",
    "    ).astype(\"string\")\n",
    "else:\n",
    "    df[\"it_spend_tier\"] = pd.Series(pd.NA, index=df.index, dtype=\"string\")\n",
    "\n",
    "device_cols = [c for c in [\n",
    "    \"no_of_pc\", \"no_of_desktops\", \"no_of_laptops\", \"no_of_routers\", \"no_of_servers\", \"no_of_storage_devices\"\n",
    "] if c in df.columns]\n",
    "\n",
    "df[\"device_total\"] = df[device_cols].sum(axis=1, min_count=1)\n",
    "df[\"log_device_total\"] = np.log1p(df[\"device_total\"])\n",
    "df[\"device_tier\"] = safe_qcut(\n",
    "    df[\"log_device_total\"], q=4, labels=[\"dev_low\", \"dev_mid\", \"dev_high\", \"dev_top\"]\n",
    ").astype(\"string\")\n",
    "\n",
    "# ---------- 5) Geography tiers ----------\n",
    "# Keep it coarse. Use region if present, else country.\n",
    "if \"region\" in df.columns:\n",
    "    df[\"geo_tier\"] = df[\"region\"].astype(\"string\")\n",
    "else:\n",
    "    df[\"geo_tier\"] = df[\"country\"].astype(\"string\")\n",
    "\n",
    "# ---------- 6) Build final segment label + id ----------\n",
    "seg_parts = [\"sic_2digit\", \"size_emp_tier\", \"size_rev_tier\", \"structure_tier\", \"it_spend_tier\", \"device_tier\", \"geo_tier\"]\n",
    "for c in seg_parts:\n",
    "    df[c] = df[c].fillna(\"Unknown\").astype(\"string\")\n",
    "\n",
    "df[\"segment_label\"] = df[seg_parts].agg(\"|\".join, axis=1)\n",
    "\n",
    "# numeric id for convenience\n",
    "seg_order = df[\"segment_label\"].value_counts().index.tolist()\n",
    "seg_map = {lab: i for i, lab in enumerate(seg_order)}\n",
    "df[\"segment_id\"] = df[\"segment_label\"].map(seg_map).astype(int)\n",
    "\n",
    "# ---------- 7) Quick sanity outputs ----------\n",
    "counts = df[\"segment_id\"].value_counts()\n",
    "print(\"Rows:\", df.shape[0])\n",
    "print(\"Num segments:\", counts.shape[0])\n",
    "print(\"Top 10 segment sizes:\\n\", counts.head(10))\n",
    "print(\"Segments with <10 companies:\", int((counts < 10).sum()))\n",
    "\n",
    "df[[\"segment_id\", \"segment_label\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7tM2QiEkBKQv",
   "metadata": {
    "id": "7tM2QiEkBKQv"
   },
   "source": [
    "# Get Secret Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "kzfVqhCkAzwZ",
   "metadata": {
    "id": "kzfVqhCkAzwZ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[32m      2\u001b[39m userdata.get(\u001b[33m'\u001b[39m\u001b[33mHF_TOKEN\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import userdata\n",
    "userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d996f3c-845c-4bcf-9039-88adae5124f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.53.0)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=5.5 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.2.4)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (8.3.1)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (25.0)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (2.3.3)\n",
      "Requirement already satisfied: pillow<13,>=7.1.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.33.4)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (23.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (2.32.5)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (3.1.46)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.5.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.26.0)\n",
      "Requirement already satisfied: narwhals>=1.27.1 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: streamlit in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.53.0)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=5.5 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.2.4)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (8.3.1)\n",
      "Collecting numpy<3,>=1.23 (from streamlit)\n",
      "  Downloading numpy-2.4.1-cp313-cp313-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (25.0)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (2.3.3)\n",
      "Requirement already satisfied: pillow<13,>=7.1.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.33.4)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (23.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (2.32.5)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (3.1.46)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.5.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.26.0)\n",
      "Requirement already satisfied: narwhals>=1.27.1 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.10.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\celes\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading numpy-2.4.1-cp313-cp313-win_amd64.whl (12.3 MB)\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/12.3 MB 1.9 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.0/12.3 MB 1.9 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.6/12.3 MB 1.8 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.8/12.3 MB 1.7 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.4/12.3 MB 1.8 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 2.6/12.3 MB 1.8 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 2.9/12.3 MB 1.7 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.1/12.3 MB 1.7 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 3.7/12.3 MB 1.7 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 4.2/12.3 MB 1.8 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 4.7/12.3 MB 1.9 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 5.2/12.3 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.8/12.3 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.0/12.3 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.1/12.3 MB 2.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 8.7/12.3 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.2/12.3 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.5/12.3 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.3/12.3 MB 3.0 MB/s  0:00:04\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bd9a52b-2089-44ff-97d8-24412d284ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   ---------------------------------------- 2/2 [openpyxl]\n",
      "\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bae9389-f10f-4f9c-8ecc-1cc31095ee82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"My First Streamlit App\")\n",
    "st.write(\"Hello, this is running from a Jupyter Notebook!\")\n",
    "\n",
    "user_input = st.text_input(\"Enter your name:\", \"Streamlit User\")\n",
    "st.write(f\"Welcome, {user_input}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c26ad86b-c8c2-46bf-bba2-13b86d196c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file created at: C:\\Users\\celes/.streamlit\\credentials.toml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the streamlit configuration directory\n",
    "streamlit_config_dir = os.path.expanduser(\"~/.streamlit\")\n",
    "os.makedirs(streamlit_config_dir, exist_ok=True)\n",
    "\n",
    "# Create the credentials.toml file with a blank email\n",
    "config_path = os.path.join(streamlit_config_dir, \"credentials.toml\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    f.write('[general]\\nemail = \"\"')\n",
    "\n",
    "print(f\"Configuration file created at: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66676ad7-04a2-45cf-892c-a468aacf1064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d5b84dd-881e-46b3-9ea2-706c2a38ec3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "st.title(\"Hello Streamlit\")\n",
    "st.sidebar.write(\"Sidebar works\")\n",
    "import subprocess, sys\n",
    "p = subprocess.Popen([sys.executable, \"-m\", \"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"])\n",
    "print(\"Started Streamlit PID:\", p.pid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "071019bc-6d46-4e51-ae24-9ef5da9031f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "\n",
    "st.set_page_config(page_title=\"Company Intelligence Explorer\", layout=\"wide\")\n",
    "st.title(\"Company Segmentation & Intelligence Explorer\")\n",
    "\n",
    "# ---------- Load (upload to avoid path issues) ----------\n",
    "uploaded = st.sidebar.file_uploader(\"Upload Excel (.xlsx)\", type=[\"xlsx\"])\n",
    "if uploaded is None:\n",
    "    st.info(\"Upload the dataset to begin.\")\n",
    "    st.stop()\n",
    "\n",
    "@st.cache_data\n",
    "def load_data(file) -> pd.DataFrame:\n",
    "    df = pd.read_excel(file)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    # standardize to snake_case-ish for your rule code\n",
    "    df.columns = [c.lower().strip().replace(\" \", \"_\").replace(\".\", \"\") for c in df.columns]\n",
    "    return df\n",
    "\n",
    "df = load_data(uploaded)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def pick_col(df, candidates):\n",
    "    lower = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        cand2 = cand.lower().strip().replace(\" \", \"_\").replace(\".\", \"\")\n",
    "        if cand2 in lower:\n",
    "            return lower[cand2]\n",
    "    return None\n",
    "\n",
    "def safe_num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "@st.cache_data\n",
    "def add_rule_segments(df_clean: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_clean.copy()\n",
    "\n",
    "    def _digits_only(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        s = str(x).strip()\n",
    "        s = re.sub(r\"\\D+\", \"\", s)\n",
    "        return s if s != \"\" else np.nan\n",
    "\n",
    "    def sic_prefix(x, n=2):\n",
    "        s = _digits_only(x)\n",
    "        if pd.isna(s):\n",
    "            return np.nan\n",
    "        return s[:n] if len(s) >= n else s.zfill(n)\n",
    "\n",
    "    def safe_qcut(series, q=4, labels=None):\n",
    "        s = pd.to_numeric(series, errors=\"coerce\")\n",
    "        if s.notna().sum() < q * 5:\n",
    "            q = 3\n",
    "            labels = labels[:3] if labels is not None else None\n",
    "        try:\n",
    "            return pd.qcut(s, q=q, labels=labels, duplicates=\"drop\")\n",
    "        except Exception:\n",
    "            return pd.Series(pd.NA, index=series.index, dtype=\"string\")\n",
    "\n",
    "    # ---------- 1) Industry bucket ----------\n",
    "    sic_col = \"8_digit_sic_code\" if \"8_digit_sic_code\" in df.columns else (\"sic_code\" if \"sic_code\" in df.columns else None)\n",
    "    df[\"sic_2digit\"] = df[sic_col].map(lambda x: sic_prefix(x, n=2)) if sic_col else \"Unknown\"\n",
    "\n",
    "    # ---------- 2) Size tiers ----------\n",
    "    for c in [\"employees_total\", \"revenue_usd\", \"it_spend\", \"it_budget\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    if \"employees_total\" in df.columns:\n",
    "        df[\"log_employees\"] = np.log1p(df[\"employees_total\"])\n",
    "        df[\"size_emp_tier\"] = safe_qcut(df[\"log_employees\"], q=4, labels=[\"emp_s\", \"emp_m\", \"emp_l\", \"emp_xl\"]).astype(\"string\")\n",
    "    else:\n",
    "        df[\"size_emp_tier\"] = \"Unknown\"\n",
    "\n",
    "    if \"revenue_usd\" in df.columns:\n",
    "        df[\"log_revenue\"] = np.log1p(df[\"revenue_usd\"])\n",
    "        df[\"size_rev_tier\"] = safe_qcut(df[\"log_revenue\"], q=4, labels=[\"rev_s\", \"rev_m\", \"rev_l\", \"rev_xl\"]).astype(\"string\")\n",
    "    else:\n",
    "        df[\"size_rev_tier\"] = \"Unknown\"\n",
    "\n",
    "    # ---------- 3) Structure tier ----------\n",
    "    for b in [\"is_headquarters\", \"is_domestic_ultimate\"]:\n",
    "        if b in df.columns:\n",
    "            if df[b].dtype.name in [\"string\", \"object\"]:\n",
    "                df[b] = df[b].astype(\"string\").str.lower().map({\"true\": True, \"false\": False})\n",
    "            df[b] = df[b].fillna(False).astype(bool)\n",
    "        else:\n",
    "            df[b] = False\n",
    "\n",
    "    df[\"has_parent_company\"] = df[\"parent_company\"].notna() if \"parent_company\" in df.columns else False\n",
    "    df[\"has_global_ultimate\"] = df[\"global_ultimate_company\"].notna() if \"global_ultimate_company\" in df.columns else False\n",
    "    df[\"has_domestic_ultimate_company\"] = df[\"domestic_ultimate_company\"].notna() if \"domestic_ultimate_company\" in df.columns else False\n",
    "\n",
    "    def structure_tier(row):\n",
    "        if row.get(\"is_headquarters\", False):\n",
    "            return \"hq\"\n",
    "        if row.get(\"is_domestic_ultimate\", False):\n",
    "            return \"domestic_ultimate\"\n",
    "        et = str(row.get(\"entity_type\", \"\")).lower()\n",
    "        if \"subsidi\" in et:\n",
    "            return \"subsidiary\"\n",
    "        if \"branch\" in et:\n",
    "            return \"branch\"\n",
    "        if row.get(\"has_parent_company\", False):\n",
    "            return \"subsidiary_like\"\n",
    "        if row.get(\"has_global_ultimate\", False) or row.get(\"has_domestic_ultimate_company\", False):\n",
    "            return \"member_of_group\"\n",
    "        return \"standalone_like\"\n",
    "\n",
    "    df[\"structure_tier\"] = df.apply(structure_tier, axis=1).astype(\"string\")\n",
    "\n",
    "    # ---------- 4) IT tiers + device tiers ----------\n",
    "    if \"it_spend\" in df.columns:\n",
    "        df[\"log_it_spend\"] = np.log1p(df[\"it_spend\"])\n",
    "        df[\"it_spend_tier\"] = safe_qcut(df[\"log_it_spend\"], q=4, labels=[\"it_low\", \"it_mid\", \"it_high\", \"it_top\"]).astype(\"string\")\n",
    "    else:\n",
    "        df[\"it_spend_tier\"] = \"Unknown\"\n",
    "\n",
    "    device_cols = [c for c in [\"no_of_pc\", \"no_of_desktops\", \"no_of_laptops\", \"no_of_routers\", \"no_of_servers\", \"no_of_storage_devices\"] if c in df.columns]\n",
    "    df[\"device_total\"] = df[device_cols].sum(axis=1, min_count=1) if device_cols else np.nan\n",
    "    df[\"log_device_total\"] = np.log1p(df[\"device_total\"])\n",
    "    df[\"device_tier\"] = safe_qcut(df[\"log_device_total\"], q=4, labels=[\"dev_low\", \"dev_mid\", \"dev_high\", \"dev_top\"]).astype(\"string\")\n",
    "\n",
    "    # ---------- 5) Geo tiers ----------\n",
    "    if \"region\" in df.columns:\n",
    "        df[\"geo_tier\"] = df[\"region\"].astype(\"string\")\n",
    "    elif \"country\" in df.columns:\n",
    "        df[\"geo_tier\"] = df[\"country\"].astype(\"string\")\n",
    "    else:\n",
    "        df[\"geo_tier\"] = \"Unknown\"\n",
    "\n",
    "    # ---------- 6) Final label/id ----------\n",
    "    seg_parts = [\"sic_2digit\", \"size_emp_tier\", \"size_rev_tier\", \"structure_tier\", \"it_spend_tier\", \"device_tier\", \"geo_tier\"]\n",
    "    for c in seg_parts:\n",
    "        df[c] = df[c].fillna(\"Unknown\").astype(\"string\")\n",
    "\n",
    "    df[\"segment_label\"] = df[seg_parts].agg(\"|\".join, axis=1)\n",
    "    seg_order = df[\"segment_label\"].value_counts().index.tolist()\n",
    "    seg_map = {lab: i for i, lab in enumerate(seg_order)}\n",
    "    df[\"segment_id\"] = df[\"segment_label\"].map(seg_map).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = add_rule_segments(df)\n",
    "\n",
    "# ---------- Sidebar filters ----------\n",
    "st.sidebar.subheader(\"Filters\")\n",
    "col_country = pick_col(df, [\"country\"])\n",
    "col_entity  = pick_col(df, [\"entity_type\"])\n",
    "col_name    = pick_col(df, [\"company_name\", \"name\", \"company\"])\n",
    "\n",
    "def multiselect_filter(label, col):\n",
    "    if col is None:\n",
    "        st.sidebar.caption(f\"⚠️ {label}: column not found\")\n",
    "        return []\n",
    "    vals = sorted([v for v in df[col].dropna().astype(str).unique()])\n",
    "    return st.sidebar.multiselect(label, vals)\n",
    "\n",
    "sel_country = multiselect_filter(\"Country\", col_country)\n",
    "sel_entity  = multiselect_filter(\"Entity Type\", col_entity)\n",
    "\n",
    "seg_vals = sorted(df[\"segment_label\"].dropna().astype(str).unique())\n",
    "sel_segs = st.sidebar.multiselect(\"Segment\", seg_vals)\n",
    "\n",
    "filtered = df.copy()\n",
    "if col_country and sel_country:\n",
    "    filtered = filtered[filtered[col_country].astype(str).isin(sel_country)]\n",
    "if col_entity and sel_entity:\n",
    "    filtered = filtered[filtered[col_entity].astype(str).isin(sel_entity)]\n",
    "if sel_segs:\n",
    "    filtered = filtered[filtered[\"segment_label\"].astype(str).isin(sel_segs)]\n",
    "\n",
    "st.sidebar.caption(f\"Filtered rows: {len(filtered):,}\")\n",
    "\n",
    "# ---------- Tabs ----------\n",
    "tab1, tab2 = st.tabs([\"Explore Companies\", \"Explore Segments\"])\n",
    "\n",
    "with tab1:\n",
    "    st.subheader(\"Company Explorer\")\n",
    "\n",
    "    if col_name:\n",
    "        company = st.selectbox(\"Select a company\", sorted(filtered[col_name].fillna(\"UNKNOWN\").astype(str).unique()))\n",
    "        row = filtered[filtered[col_name].astype(str) == str(company)].head(1)\n",
    "        st.write(\"Company record\")\n",
    "        st.dataframe(row, use_container_width=True)\n",
    "\n",
    "    st.subheader(\"Filtered preview (top 200 rows)\")\n",
    "    st.dataframe(filtered.head(200), use_container_width=True)\n",
    "\n",
    "with tab2:\n",
    "    st.subheader(\"Segment Summary\")\n",
    "    seg_counts = filtered[\"segment_label\"].value_counts().reset_index()\n",
    "    seg_counts.columns = [\"segment_label\", \"count\"]\n",
    "    st.dataframe(seg_counts.head(30), use_container_width=True)\n",
    "\n",
    "    st.subheader(\"Top segments chart\")\n",
    "    top = seg_counts.head(15)\n",
    "    fig = plt.figure()\n",
    "    plt.bar(top[\"segment_label\"].astype(str), top[\"count\"])\n",
    "    plt.xticks(rotation=90)\n",
    "    st.pyplot(fig, clear_figure=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0f91773-c8bf-4e2d-b225-e118c6fcc497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open: http://localhost:8502  |  PID: 27028\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "p = subprocess.Popen([sys.executable, \"-m\", \"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8502\", \"--server.headless\", \"true\"])\n",
    "print(\"Open: http://localhost:8502  |  PID:\", p.pid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5383f311-af21-4ed5-83d1-2f00a16e2b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPEN THIS: http://localhost:51555 | PID: 32092\n",
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:51555\n",
      "  Network URL: http://10.249.51.38:51555\n",
      "  External URL: http://137.132.26.219:51555\n",
      "\n",
      "2026-01-19 13:28:31.551 Uncaught app execution\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\celes\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\streamlit\\runtime\\scriptrunner\\exec_code.py\", line 129, in exec_func_with_error_handling\n",
      "    result = func()\n",
      "  File \"C:\\Users\\celes\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\streamlit\\runtime\\scriptrunner\\script_runner.py\", line 672, in code_to_exec\n",
      "    exec(code, module.__dict__)  # noqa: S102\n",
      "    ~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\celes\\Documents\\GitHub\\datathon-2s2d\\app.py\", line 150, in <module>\n",
      "    df = add_rule_segments(df)\n",
      "  File \"C:\\Users\\celes\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py\", line 281, in __call__\n",
      "    return self._get_or_create_cached_value(args, kwargs, spinner_message)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\celes\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py\", line 326, in _get_or_create_cached_value\n",
      "    return self._handle_cache_miss(cache, value_key, func_args, func_kwargs)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\celes\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\streamlit\\runtime\\caching\\cache_utils.py\", line 385, in _handle_cache_miss\n",
      "    computed_value = self._info.func(*func_args, **func_kwargs)\n",
      "  File \"C:\\Users\\celes\\Documents\\GitHub\\datathon-2s2d\\app.py\", line 126, in add_rule_segments\n",
      "    df[\"device_total\"] = df[device_cols].sum(axis=1, min_count=1) if device_cols else np.nan\n",
      "                         ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\celes\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py\", line 11697, in sum\n",
      "    result = super().sum(axis, skipna, numeric_only, min_count, **kwargs)\n",
      "  File \"C:\\Users\\celes\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py\", line 12571, in sum\n"
     ]
    }
   ],
   "source": [
    "import socket, subprocess, sys, time, webbrowser\n",
    "\n",
    "def get_free_port():\n",
    "    s = socket.socket()\n",
    "    s.bind((\"\", 0))\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "port = get_free_port()\n",
    "\n",
    "p = subprocess.Popen(\n",
    "    [sys.executable, \"-m\", \"streamlit\", \"run\", \"app.py\",\n",
    "     \"--server.port\", str(port),\n",
    "     \"--server.headless\", \"true\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "time.sleep(1)\n",
    "url = f\"http://localhost:{port}\"\n",
    "print(\"OPEN THIS:\", url, \"| PID:\", p.pid)\n",
    "\n",
    "# try to open browser\n",
    "webbrowser.open(url)\n",
    "\n",
    "# print first ~30 log lines so you can see if it crashed\n",
    "for _ in range(30):\n",
    "    line = p.stdout.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    print(line, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69deaffe-6fbd-4186-9ecb-c0ec07771613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
