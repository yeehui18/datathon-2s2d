{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "N_smoqVgjUOW",
   "metadata": {
    "id": "N_smoqVgjUOW"
   },
   "source": [
    "# **NUS DATHATON 2026**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cwbDfwIyjhXv",
   "metadata": {
    "id": "cwbDfwIyjhXv"
   },
   "source": [
    "***2 Sons 2 Daughters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d996f3c-845c-4bcf-9039-88adae5124f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (1.53.0)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=5.5 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (6.2.4)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (8.3.1)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (25.0)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (2.3.3)\n",
      "Requirement already satisfied: pillow<13,>=7.1.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (12.1.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (6.33.4)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (23.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (2.32.5)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (4.15.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (3.1.46)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from streamlit) (6.5.4)\n",
      "Requirement already satisfied: jinja2 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.26.0)\n",
      "Requirement already satisfied: narwhals>=1.27.1 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.15.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2026.1.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: openpyxl in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from openpyxl) (2.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: huggingface_hub in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (1.3.2)\n",
      "Requirement already satisfied: filelock in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from huggingface_hub) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from huggingface_hub) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from huggingface_hub) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from huggingface_hub) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from huggingface_hub) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: anyio in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/jeffersonjuhardi/.venv/lib/python3.12/site-packages (from typer-slim->huggingface_hub) (8.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit\n",
    "!pip install openpyxl\n",
    "!pip install matplotlib\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c26ad86b-c8c2-46bf-bba2-13b86d196c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file created at: /Users/jeffersonjuhardi/.streamlit/credentials.toml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the streamlit configuration directory\n",
    "streamlit_config_dir = os.path.expanduser(\"~/.streamlit\")\n",
    "os.makedirs(streamlit_config_dir, exist_ok=True)\n",
    "\n",
    "# Create the credentials.toml file with a blank email\n",
    "config_path = os.path.join(streamlit_config_dir, \"credentials.toml\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    f.write('[general]\\nemail = \"\"')\n",
    "\n",
    "print(f\"Configuration file created at: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "071019bc-6d46-4e51-ae24-9ef5da9031f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Optional AI\n",
    "try:\n",
    "    from huggingface_hub import InferenceClient\n",
    "    HF_AVAILABLE = True\n",
    "except Exception:\n",
    "    HF_AVAILABLE = False\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Page config\n",
    "# =========================\n",
    "st.set_page_config(page_title=\"Company Intelligence Explorer\", layout=\"wide\")\n",
    "st.title(\"Company Segmentation and Intelligence Explorer\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Upload\n",
    "# =========================\n",
    "uploaded = st.sidebar.file_uploader(\"Upload Excel (.xlsx)\", type=[\"xlsx\"])\n",
    "if uploaded is None:\n",
    "    st.info(\"Upload the dataset to begin.\")\n",
    "    st.stop()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utility helpers\n",
    "# =========================\n",
    "def to_snake(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"[^\\w]+\", \"_\", s)\n",
    "    return s.strip(\"_\")\n",
    "\n",
    "\n",
    "def normalise_missing_text(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Normalise common missing tokens into NA.\n",
    "    Keeps dtype as plain object.\n",
    "    \"\"\"\n",
    "    s = series.astype(object)\n",
    "    miss = {\"\", \"na\", \"n/a\", \"none\", \"null\", \"unknown\", \"nan\"}\n",
    "    out = []\n",
    "    for v in s.values:\n",
    "        if pd.isna(v):\n",
    "            out.append(pd.NA)\n",
    "            continue\n",
    "        t = str(v).strip()\n",
    "        out.append(pd.NA if t.lower() in miss else t)\n",
    "    return pd.Series(out, index=series.index, dtype=object)\n",
    "\n",
    "\n",
    "def clean_phone(x):\n",
    "    \"\"\"\n",
    "    Excel sometimes stores phone numbers as floats/scientific notation.\n",
    "    Convert to a clean digits string where possible.\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return pd.NA\n",
    "    s = str(x).strip()\n",
    "\n",
    "    try:\n",
    "        f = float(s)\n",
    "        if np.isfinite(f):\n",
    "            return str(int(f))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if re.match(r\"^\\d+\\.0$\", s):\n",
    "        return s[:-2]\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def bucket_to_midpoint(x):\n",
    "    \"\"\"\n",
    "    Parse bucket ranges commonly found in device/server columns.\n",
    "    Examples:\n",
    "      '1 to 10' -> 5.5\n",
    "      '1,001 to 5,000' -> 3000.5\n",
    "      '100000+' -> 100000\n",
    "      '12' -> 12\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower().replace(\",\", \"\")\n",
    "    if s in {\"\", \"na\", \"n/a\", \"none\", \"null\", \"unknown\"}:\n",
    "        return np.nan\n",
    "\n",
    "    m = re.match(r\"^(\\d+)\\s*\\+$\", s)\n",
    "    if m:\n",
    "        return float(m.group(1))\n",
    "\n",
    "    m = re.match(r\"^(\\d+)\\s*(to|-)\\s*(\\d+)$\", s)\n",
    "    if m:\n",
    "        a, b = float(m.group(1)), float(m.group(3))\n",
    "        return (a + b) / 2.0\n",
    "\n",
    "    m = re.match(r\"^\\d+(\\.\\d+)?$\", s)\n",
    "    if m:\n",
    "        return float(s)\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def safe_numeric(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert to numeric safely.\n",
    "    If most values fail numeric conversion, attempt bucket_to_midpoint parsing.\n",
    "    \"\"\"\n",
    "    x = pd.to_numeric(series, errors=\"coerce\")\n",
    "    if x.notna().mean() < 0.30:\n",
    "        x2 = series.map(bucket_to_midpoint)\n",
    "        if pd.Series(x2).notna().mean() > x.notna().mean():\n",
    "            return pd.to_numeric(x2, errors=\"coerce\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def zero_to_nan(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert 0 -> NaN for columns where 0 is likely a missing placeholder.\n",
    "    \"\"\"\n",
    "    x = pd.to_numeric(series, errors=\"coerce\")\n",
    "    return x.mask(x == 0, np.nan)\n",
    "\n",
    "\n",
    "def pick_col(df: pd.DataFrame, candidates):\n",
    "    \"\"\"\n",
    "    Safe picker: returns first matching column, or None.\n",
    "    Never crashes even if df is None.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    if not hasattr(df, \"columns\"):\n",
    "        return None\n",
    "    cols = set(df.columns)\n",
    "    for cand in candidates:\n",
    "        if cand in cols:\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "\n",
    "def robust_z(x: pd.Series) -> pd.Series:\n",
    "    v = pd.to_numeric(x, errors=\"coerce\")\n",
    "    med = np.nanmedian(v)\n",
    "    mad = np.nanmedian(np.abs(v - med))\n",
    "    if not np.isfinite(mad) or mad == 0:\n",
    "        return pd.Series(np.nan, index=x.index)\n",
    "    return (v - med) / (1.4826 * mad)\n",
    "\n",
    "\n",
    "def percentile_within(group: pd.Series, value: float) -> float:\n",
    "    g = pd.to_numeric(group, errors=\"coerce\").dropna()\n",
    "    if len(g) == 0 or not np.isfinite(value):\n",
    "        return np.nan\n",
    "    return float((g <= value).mean() * 100.0)\n",
    "\n",
    "\n",
    "def safe_log1p(s: pd.Series) -> pd.Series:\n",
    "    x = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return np.log1p(x)\n",
    "\n",
    "\n",
    "def digits_only(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    return re.sub(r\"\\D+\", \"\", str(x))\n",
    "\n",
    "\n",
    "def sic_prefix(x, n=2):\n",
    "    s = digits_only(x)\n",
    "    if not s:\n",
    "        return \"Unknown\"\n",
    "    if len(s) < n:\n",
    "        s = s.zfill(n)\n",
    "    return s[:n]\n",
    "\n",
    "\n",
    "def build_display_name(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Build a human-friendly label for UI dropdowns.\n",
    "    IMPORTANT: prioritises company_sites (your dataset's \"company name\" column).\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        \"company_sites\",          # <-- treat as primary company name\n",
    "        \"company_name\",\n",
    "        \"name\",\n",
    "        \"company\",\n",
    "        \"website\",\n",
    "        \"duns_number\",\n",
    "        \"address_line_1\",\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            s = df[c].astype(object)\n",
    "            out = s.apply(lambda v: str(v).strip() if not pd.isna(v) else \"\")\n",
    "            if (out != \"\").mean() > 0.30:\n",
    "                out = out.replace({\"\": \"UNKNOWN\"})\n",
    "                return out\n",
    "    return pd.Series([\"UNKNOWN\"] * len(df), index=df.index, dtype=object)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Load and clean\n",
    "# =========================\n",
    "@st.cache_data\n",
    "def load_and_clean(file) -> pd.DataFrame:\n",
    "    df = pd.read_excel(file)\n",
    "    df.columns = [to_snake(c) for c in df.columns]\n",
    "\n",
    "    # --- Dedup: exact duplicates ---\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    after_exact = len(df)\n",
    "\n",
    "    # --- Dedup: by DUNS, keep most complete row ---\n",
    "    if \"duns_number\" in df.columns:\n",
    "        completeness = df.notna().sum(axis=1)\n",
    "        df = df.assign(_completeness=completeness).sort_values(\"_completeness\", ascending=False)\n",
    "        df = df.drop_duplicates(subset=[\"duns_number\"], keep=\"first\").drop(columns=[\"_completeness\"])\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # --- Normalise content text fields ---\n",
    "    for c in [\n",
    "        \"company_sites\",\n",
    "        \"website\",\n",
    "        \"address_line_1\",\n",
    "        \"country\", \"city\", \"state\", \"state_or_province_abbreviation\",\n",
    "        \"entity_type\", \"ownership_type\", \"legal_status\",\n",
    "        \"company_status_active_inactive\",\n",
    "        \"sic_description\", \"8_digit_sic_description\",\n",
    "        \"parent_company\", \"global_ultimate_company\", \"domestic_ultimate_company\",\n",
    "    ]:\n",
    "        if c in df.columns:\n",
    "            df[c] = normalise_missing_text(df[c])\n",
    "\n",
    "    # Country: case normalisation\n",
    "    if \"country\" in df.columns:\n",
    "        df[\"country\"] = df[\"country\"].astype(object).apply(lambda x: str(x).upper().strip() if not pd.isna(x) else pd.NA)\n",
    "\n",
    "    # Entity type: consistent casing for UI\n",
    "    if \"entity_type\" in df.columns:\n",
    "        df[\"entity_type\"] = df[\"entity_type\"].astype(object).apply(lambda x: str(x).strip().title() if not pd.isna(x) else pd.NA)\n",
    "\n",
    "    # Phone number\n",
    "    if \"phone_number\" in df.columns:\n",
    "        df[\"phone_number\"] = df[\"phone_number\"].apply(clean_phone)\n",
    "\n",
    "    # --- Numeric conversions and placeholder zeros ---\n",
    "    placeholder_zero_cols = [\n",
    "        \"employees_total\", \"employees_single_site\",\n",
    "        \"revenue_usd\", \"market_value_usd\",\n",
    "        \"it_budget\", \"it_spend\",\n",
    "        \"corporate_family_members\"\n",
    "    ]\n",
    "    for c in placeholder_zero_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = zero_to_nan(df[c])\n",
    "\n",
    "    # Year found sanity\n",
    "    if \"year_found\" in df.columns:\n",
    "        y = pd.to_numeric(df[\"year_found\"], errors=\"coerce\")\n",
    "        df[\"year_found\"] = y.mask((y <= 1700) | (y > 2026), np.nan)\n",
    "\n",
    "    # --- Device/server columns: parse buckets -> numeric midpoints, keep raw bucket too ---\n",
    "    device_cols = [\n",
    "        \"no_of_pc\", \"no_of_desktops\", \"no_of_laptops\",\n",
    "        \"no_of_routers\", \"no_of_servers\", \"no_of_storage_devices\"\n",
    "    ]\n",
    "    for c in device_cols:\n",
    "        if c in df.columns:\n",
    "            df[c + \"_bucket\"] = normalise_missing_text(df[c])  # raw bucket string for UI\n",
    "            df[c] = safe_numeric(df[c])                        # numeric midpoints for modelling\n",
    "\n",
    "    dev_present = [c for c in device_cols if c in df.columns]\n",
    "    df[\"device_total\"] = df[dev_present].sum(axis=1, min_count=1) if dev_present else np.nan\n",
    "\n",
    "    # --- Derived metrics for insights ---\n",
    "    if \"revenue_usd\" in df.columns and \"employees_total\" in df.columns:\n",
    "        df[\"revenue_per_employee\"] = df[\"revenue_usd\"] / df[\"employees_total\"]\n",
    "\n",
    "    if \"it_spend\" in df.columns and \"revenue_usd\" in df.columns:\n",
    "        df[\"it_spend_to_revenue\"] = df[\"it_spend\"] / df[\"revenue_usd\"]\n",
    "\n",
    "    if \"it_spend\" in df.columns and \"employees_total\" in df.columns:\n",
    "        df[\"it_spend_per_employee\"] = df[\"it_spend\"] / df[\"employees_total\"]\n",
    "\n",
    "    if \"no_of_servers\" in df.columns and \"device_total\" in df.columns:\n",
    "        df[\"server_to_device_ratio\"] = df[\"no_of_servers\"] / df[\"device_total\"]\n",
    "\n",
    "    if \"no_of_laptops\" in df.columns and \"device_total\" in df.columns:\n",
    "        df[\"laptop_to_device_ratio\"] = df[\"no_of_laptops\"] / df[\"device_total\"]\n",
    "\n",
    "    if \"no_of_desktops\" in df.columns and \"device_total\" in df.columns:\n",
    "        df[\"desktop_to_device_ratio\"] = df[\"no_of_desktops\"] / df[\"device_total\"]\n",
    "\n",
    "    # --- Keep only columns relevant to objectives (5 attribute groups + UI) ---\n",
    "    industry_cols = [\"sic_code\", \"sic_description\", \"8_digit_sic_code\", \"8_digit_sic_description\", \"naics_code\", \"naics_description\"]\n",
    "    size_cols = [\"employees_single_site\", \"employees_total\", \"revenue_usd\", \"market_value_usd\", \"year_found\", \"revenue_per_employee\"]\n",
    "    structure_cols = [\n",
    "        \"entity_type\", \"ownership_type\", \"corporate_family_members\",\n",
    "        \"is_headquarters\", \"is_domestic_ultimate\",\n",
    "        \"parent_company\", \"global_ultimate_company\", \"domestic_ultimate_company\"\n",
    "    ]\n",
    "    it_cols = [\n",
    "        \"it_budget\", \"it_spend\", \"it_spend_to_revenue\", \"it_spend_per_employee\",\n",
    "        \"device_total\", \"server_to_device_ratio\", \"laptop_to_device_ratio\", \"desktop_to_device_ratio\"\n",
    "    ] + device_cols + [c + \"_bucket\" for c in device_cols if c in df.columns]\n",
    "    geo_cols = [\"country\", \"state\", \"state_or_province_abbreviation\", \"city\", \"postal_code\", \"lattitude\", \"longitude\"]\n",
    "    ui_cols = [\n",
    "        \"duns_number\",\n",
    "        \"company_sites\",\n",
    "        \"website\",\n",
    "        \"address_line_1\",\n",
    "        \"phone_number\",\n",
    "        \"registration_number\",\n",
    "        \"company_description\",\n",
    "        \"company_status_active_inactive\",\n",
    "        \"legal_status\",\n",
    "    ]\n",
    "\n",
    "    keep = []\n",
    "    for grp in [industry_cols, size_cols, structure_cols, it_cols, geo_cols, ui_cols]:\n",
    "        keep += [c for c in grp if c in df.columns]\n",
    "    keep = list(dict.fromkeys(keep))\n",
    "    df = df[keep].copy()\n",
    "\n",
    "    # --- Add display_name for dropdowns (company_sites first) ---\n",
    "    df[\"display_name\"] = build_display_name(df)\n",
    "\n",
    "    # Store dedup stats\n",
    "    df.attrs[\"dedup_before\"] = before\n",
    "    df.attrs[\"dedup_after_exact\"] = after_exact\n",
    "    df.attrs[\"dedup_after_key\"] = len(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "raw_df = load_and_clean(uploaded)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Segmentation\n",
    "# =========================\n",
    "@st.cache_data\n",
    "def add_rule_segments(df_in: pd.DataFrame, min_industry_count: int, simple_segments: bool) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Interpretable segments based on:\n",
    "    - Industry bucket (SIC 2-digit, rare -> Other)\n",
    "    - Size tiers (employees, revenue)\n",
    "    - Structure tier (HQ / domestic ultimate / subsidiary / branch / etc.)\n",
    "    - IT footprint tiers (it_spend, device_total)\n",
    "    - Geography (country) if full mode\n",
    "\n",
    "    Adds:\n",
    "    - segment_label (raw, pipe-separated)\n",
    "    - segment_description (human-readable, with Undisclosed + field names)\n",
    "    \"\"\"\n",
    "    if df_in is None:\n",
    "        return None\n",
    "\n",
    "    df = df_in.copy()\n",
    "\n",
    "    # Industry bucket\n",
    "    sic_col = \"8_digit_sic_code\" if \"8_digit_sic_code\" in df.columns else (\"sic_code\" if \"sic_code\" in df.columns else None)\n",
    "    if sic_col:\n",
    "        df[\"sic_2digit\"] = df[sic_col].map(lambda x: sic_prefix(x, 2))\n",
    "    else:\n",
    "        df[\"sic_2digit\"] = \"Unknown\"\n",
    "\n",
    "    vc = df[\"sic_2digit\"].value_counts(dropna=False)\n",
    "    rare = vc[vc < int(min_industry_count)].index\n",
    "    df[\"sic_bucket\"] = df[\"sic_2digit\"].where(~df[\"sic_2digit\"].isin(rare), \"Other\")\n",
    "    df[\"sic_bucket\"] = df[\"sic_bucket\"].fillna(\"Unknown\")\n",
    "\n",
    "    # Safer qcut using ranks\n",
    "    def qcut_rank(series: pd.Series, labels):\n",
    "        x = pd.to_numeric(series, errors=\"coerce\")\n",
    "        if x.notna().sum() < 40:\n",
    "            return pd.Series([\"Unknown\"] * len(series), index=series.index, dtype=object)\n",
    "        r = x.rank(method=\"average\")\n",
    "        try:\n",
    "            return pd.qcut(r, q=len(labels), labels=labels, duplicates=\"drop\").astype(object)\n",
    "        except Exception:\n",
    "            return pd.Series([\"Unknown\"] * len(series), index=series.index, dtype=object)\n",
    "\n",
    "    # Size tiers\n",
    "    df[\"size_emp_tier\"] = qcut_rank(safe_log1p(df[\"employees_total\"]), [\"emp_s\", \"emp_m\", \"emp_l\", \"emp_xl\"]) if \"employees_total\" in df.columns else \"Unknown\"\n",
    "    df[\"size_rev_tier\"] = qcut_rank(safe_log1p(df[\"revenue_usd\"]), [\"rev_s\", \"rev_m\", \"rev_l\", \"rev_xl\"]) if \"revenue_usd\" in df.columns else \"Unknown\"\n",
    "\n",
    "    # Structure tier\n",
    "    def to_bool_col(colname):\n",
    "        if colname not in df.columns:\n",
    "            return pd.Series([False] * len(df), index=df.index, dtype=bool)\n",
    "        s = df[colname].astype(object)\n",
    "        out = []\n",
    "        for v in s.values:\n",
    "            if pd.isna(v):\n",
    "                out.append(False)\n",
    "            else:\n",
    "                t = str(v).strip().lower()\n",
    "                out.append(True if t == \"true\" else False)\n",
    "        return pd.Series(out, index=df.index, dtype=bool)\n",
    "\n",
    "    is_hq = to_bool_col(\"is_headquarters\")\n",
    "    is_du = to_bool_col(\"is_domestic_ultimate\")\n",
    "\n",
    "    df[\"has_parent_company\"] = df[\"parent_company\"].notna() if \"parent_company\" in df.columns else False\n",
    "    df[\"has_global_ultimate\"] = df[\"global_ultimate_company\"].notna() if \"global_ultimate_company\" in df.columns else False\n",
    "    df[\"has_domestic_ultimate_company\"] = df[\"domestic_ultimate_company\"].notna() if \"domestic_ultimate_company\" in df.columns else False\n",
    "\n",
    "    if \"entity_type\" in df.columns:\n",
    "        et = df[\"entity_type\"].astype(object).apply(lambda x: str(x).lower() if not pd.isna(x) else \"\")\n",
    "    else:\n",
    "        et = pd.Series([\"\"] * len(df), index=df.index, dtype=object)\n",
    "\n",
    "    df[\"structure_tier\"] = np.select(\n",
    "        [\n",
    "            is_hq,\n",
    "            is_du,\n",
    "            et.str.contains(\"subsidi\", na=False),\n",
    "            et.str.contains(\"branch\", na=False),\n",
    "            df[\"has_parent_company\"] == True,\n",
    "            (df[\"has_global_ultimate\"] == True) | (df[\"has_domestic_ultimate_company\"] == True),\n",
    "        ],\n",
    "        [\"hq\", \"domestic_ultimate\", \"subsidiary\", \"branch\", \"subsidiary_like\", \"member_of_group\"],\n",
    "        default=\"standalone_like\"\n",
    "    ).astype(object)\n",
    "\n",
    "    # IT tiers\n",
    "    df[\"it_spend_tier\"] = qcut_rank(safe_log1p(df[\"it_spend\"]), [\"it_low\", \"it_mid\", \"it_high\", \"it_top\"]) if \"it_spend\" in df.columns else \"Unknown\"\n",
    "    df[\"device_tier\"] = qcut_rank(safe_log1p(df[\"device_total\"]), [\"dev_low\", \"dev_mid\", \"dev_high\", \"dev_top\"]) if \"device_total\" in df.columns else \"Unknown\"\n",
    "\n",
    "    # Geo tier\n",
    "    df[\"geo_tier\"] = df[\"country\"].fillna(\"Unknown\") if \"country\" in df.columns else \"Unknown\"\n",
    "\n",
    "    # Segment label\n",
    "    if simple_segments:\n",
    "        seg_parts = [\"sic_bucket\", \"size_emp_tier\", \"size_rev_tier\", \"it_spend_tier\", \"device_tier\"]\n",
    "    else:\n",
    "        seg_parts = [\"sic_bucket\", \"size_emp_tier\", \"size_rev_tier\", \"structure_tier\", \"it_spend_tier\", \"device_tier\", \"geo_tier\"]\n",
    "\n",
    "    for c in seg_parts:\n",
    "        df[c] = df[c].astype(object)\n",
    "        df[c] = df[c].where(~pd.isna(df[c]), \"Unknown\")\n",
    "\n",
    "    df[\"segment_label\"] = df[seg_parts].astype(str).agg(\"|\".join, axis=1)\n",
    "    df[\"segment_id\"] = df[\"segment_label\"].astype(\"category\").cat.codes\n",
    "\n",
    "    # ---------- Human readable segment description (with field names for Undisclosed) ----------\n",
    "    emp_map = {\n",
    "        \"emp_s\": \"small employee size\",\n",
    "        \"emp_m\": \"medium employee size\",\n",
    "        \"emp_l\": \"large employee size\",\n",
    "        \"emp_xl\": \"very large employee size\",\n",
    "    }\n",
    "    rev_map = {\n",
    "        \"rev_s\": \"small revenue tier\",\n",
    "        \"rev_m\": \"medium revenue tier\",\n",
    "        \"rev_l\": \"large revenue tier\",\n",
    "        \"rev_xl\": \"very large revenue tier\",\n",
    "    }\n",
    "    it_map = {\n",
    "        \"it_low\": \"low IT spend tier\",\n",
    "        \"it_mid\": \"medium IT spend tier\",\n",
    "        \"it_high\": \"high IT spend tier\",\n",
    "        \"it_top\": \"top IT spend tier\",\n",
    "    }\n",
    "    dev_map = {\n",
    "        \"dev_low\": \"low infrastructure scale\",\n",
    "        \"dev_mid\": \"medium infrastructure scale\",\n",
    "        \"dev_high\": \"high infrastructure scale\",\n",
    "        \"dev_top\": \"top infrastructure scale\",\n",
    "    }\n",
    "    struct_map = {\n",
    "        \"hq\": \"headquarters\",\n",
    "        \"domestic_ultimate\": \"domestic ultimate\",\n",
    "        \"subsidiary\": \"subsidiary\",\n",
    "        \"branch\": \"branch\",\n",
    "        \"subsidiary_like\": \"subsidiary like (has parent)\",\n",
    "        \"member_of_group\": \"member of a corporate group\",\n",
    "        \"standalone_like\": \"standalone like\",\n",
    "    }\n",
    "\n",
    "    def _is_undisclosed(v):\n",
    "        if pd.isna(v):\n",
    "            return True\n",
    "        s = str(v).strip()\n",
    "        if s == \"\":\n",
    "            return True\n",
    "        if s.lower() == \"unknown\":\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _pretty_industry(row):\n",
    "        # Prefer sic_2digit for detail; fall back to sic_bucket\n",
    "        s2 = row.get(\"sic_2digit\", \"Unknown\")\n",
    "        if _is_undisclosed(s2):\n",
    "            sb = row.get(\"sic_bucket\", \"Unknown\")\n",
    "            if _is_undisclosed(sb):\n",
    "                return \"Industry: undisclosed\"\n",
    "            return f\"Industry: {sb}\"\n",
    "        return f\"Industry: SIC prefix {str(s2)}\"\n",
    "\n",
    "    def _pretty_size_emp(row):\n",
    "        v = row.get(\"size_emp_tier\", \"Unknown\")\n",
    "        if _is_undisclosed(v):\n",
    "            return \"Employees: undisclosed\"\n",
    "        return \"Employees: \" + emp_map.get(str(v), str(v))\n",
    "\n",
    "    def _pretty_size_rev(row):\n",
    "        v = row.get(\"size_rev_tier\", \"Unknown\")\n",
    "        if _is_undisclosed(v):\n",
    "            return \"Revenue: undisclosed\"\n",
    "        return \"Revenue: \" + rev_map.get(str(v), str(v))\n",
    "\n",
    "    def _pretty_structure(row):\n",
    "        v = row.get(\"structure_tier\", \"Unknown\")\n",
    "        if _is_undisclosed(v):\n",
    "            return \"Structure: undisclosed\"\n",
    "        return \"Structure: \" + struct_map.get(str(v), str(v))\n",
    "\n",
    "    def _pretty_it(row):\n",
    "        v = row.get(\"it_spend_tier\", \"Unknown\")\n",
    "        if _is_undisclosed(v):\n",
    "            return \"IT spend: undisclosed\"\n",
    "        return \"IT spend: \" + it_map.get(str(v), str(v))\n",
    "\n",
    "    def _pretty_devices(row):\n",
    "        v = row.get(\"device_tier\", \"Unknown\")\n",
    "        if _is_undisclosed(v):\n",
    "            return \"Devices: undisclosed\"\n",
    "        return \"Devices: \" + dev_map.get(str(v), str(v))\n",
    "\n",
    "    def _pretty_geo(row):\n",
    "        v = row.get(\"geo_tier\", \"Unknown\")\n",
    "        if _is_undisclosed(v):\n",
    "            return \"Geography: undisclosed\"\n",
    "        return \"Geography: \" + str(v)\n",
    "\n",
    "    def _segment_sentence(row):\n",
    "        parts = []\n",
    "        parts.append(_pretty_industry(row))\n",
    "        parts.append(_pretty_size_emp(row))\n",
    "        parts.append(_pretty_size_rev(row))\n",
    "        if not simple_segments:\n",
    "            parts.append(_pretty_structure(row))\n",
    "        parts.append(_pretty_it(row))\n",
    "        parts.append(_pretty_devices(row))\n",
    "        if not simple_segments:\n",
    "            parts.append(_pretty_geo(row))\n",
    "        return \"; \".join(parts) + \".\"\n",
    "\n",
    "    df[\"segment_description\"] = df.apply(_segment_sentence, axis=1).astype(object)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Sidebar segmentation controls\n",
    "st.sidebar.subheader(\"Segmentation settings\")\n",
    "simple_segments = st.sidebar.toggle(\"Simple segments (recommended)\", value=True)\n",
    "min_industry_count = st.sidebar.slider(\"Min companies per industry bucket\", 10, 300, 80, 10)\n",
    "\n",
    "df = add_rule_segments(raw_df, min_industry_count=min_industry_count, simple_segments=simple_segments)\n",
    "\n",
    "# Guard: if segmentation accidentally returns None, stop cleanly (prevents NoneType crashes)\n",
    "if df is None:\n",
    "    st.error(\"Segmentation returned None. This usually means add_rule_segments() did not reach `return df` due to indentation or an exception.\")\n",
    "    st.stop()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Filters\n",
    "# =========================\n",
    "st.sidebar.subheader(\"Filters\")\n",
    "col_country = pick_col(df, [\"country\"])\n",
    "col_entity = pick_col(df, [\"entity_type\"])\n",
    "col_state = pick_col(df, [\"state\"])\n",
    "col_city = pick_col(df, [\"city\"])\n",
    "\n",
    "def multiselect_filter(label, col):\n",
    "    if col is None:\n",
    "        st.sidebar.caption(f\"{label}: column not found\")\n",
    "        return []\n",
    "    vals = sorted([v for v in df[col].dropna().astype(str).unique()])\n",
    "    return st.sidebar.multiselect(label, vals)\n",
    "\n",
    "sel_country = multiselect_filter(\"Country\", col_country)\n",
    "sel_entity = multiselect_filter(\"Entity type\", col_entity)\n",
    "sel_state = multiselect_filter(\"State\", col_state)\n",
    "sel_city = multiselect_filter(\"City\", col_city)\n",
    "\n",
    "seg_vals = sorted(df[\"segment_label\"].dropna().astype(str).unique())\n",
    "sel_segs = st.sidebar.multiselect(\"Segment (raw label)\", seg_vals)\n",
    "\n",
    "filtered = df.copy()\n",
    "if col_country and sel_country:\n",
    "    filtered = filtered[filtered[col_country].astype(str).isin(sel_country)]\n",
    "if col_entity and sel_entity:\n",
    "    filtered = filtered[filtered[col_entity].astype(str).isin(sel_entity)]\n",
    "if col_state and sel_state:\n",
    "    filtered = filtered[filtered[col_state].astype(str).isin(sel_state)]\n",
    "if col_city and sel_city:\n",
    "    filtered = filtered[filtered[col_city].astype(str).isin(sel_city)]\n",
    "if sel_segs:\n",
    "    filtered = filtered[filtered[\"segment_label\"].astype(str).isin(sel_segs)]\n",
    "\n",
    "st.sidebar.caption(f\"Filtered rows: {len(filtered):,}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Segment profiling (no merge collisions)\n",
    "# =========================\n",
    "@st.cache_data\n",
    "def build_segment_profiles(d: pd.DataFrame) -> pd.DataFrame:\n",
    "    metrics = [c for c in [\n",
    "        \"employees_total\", \"revenue_usd\", \"market_value_usd\",\n",
    "        \"it_budget\", \"it_spend\", \"device_total\",\n",
    "        \"it_spend_to_revenue\", \"it_spend_per_employee\",\n",
    "        \"corporate_family_members\",\n",
    "        \"server_to_device_ratio\", \"laptop_to_device_ratio\", \"desktop_to_device_ratio\"\n",
    "    ] if c in d.columns]\n",
    "\n",
    "    agg = {m: \"median\" for m in metrics}\n",
    "    prof = d.groupby([\"segment_id\", \"segment_label\"], dropna=False).agg(agg)\n",
    "    prof[\"count\"] = d.groupby([\"segment_id\", \"segment_label\"], dropna=False).size()\n",
    "    prof = prof.reset_index().sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Add segment_description (first non-null per segment_id)\n",
    "    if \"segment_description\" in d.columns:\n",
    "        desc_map = d.groupby(\"segment_id\")[\"segment_description\"].apply(lambda s: s.dropna().iloc[0] if len(s.dropna()) else \"\")\n",
    "        prof[\"segment_description\"] = prof[\"segment_id\"].map(desc_map)\n",
    "    else:\n",
    "        prof[\"segment_description\"] = \"\"\n",
    "\n",
    "    def top_cat_and_share(df_seg: pd.DataFrame, col: str):\n",
    "        s = df_seg[col].fillna(\"Unknown\").astype(str)\n",
    "        vc = s.value_counts()\n",
    "        if len(vc) == 0:\n",
    "            return (\"Unknown\", np.nan)\n",
    "        top = vc.index[0]\n",
    "        share = float(vc.iloc[0] / vc.sum() * 100.0)\n",
    "        return (top, share)\n",
    "\n",
    "    comp_cols = []\n",
    "    for col in [\n",
    "        \"country\", \"entity_type\", \"sic_description\", \"8_digit_sic_description\",\n",
    "        \"sic_bucket\", \"structure_tier\", \"state\", \"city\"\n",
    "    ]:\n",
    "        if col in d.columns:\n",
    "            comp_cols.append(col)\n",
    "\n",
    "    comp_rows = []\n",
    "    for seg_id, df_seg in d.groupby(\"segment_id\", dropna=False):\n",
    "        row = {\"segment_id\": int(seg_id)}\n",
    "        for col in comp_cols:\n",
    "            top, share = top_cat_and_share(df_seg, col)\n",
    "            row[f\"top_{col}\"] = top\n",
    "            row[f\"top_{col}_share\"] = round(share, 1) if np.isfinite(share) else np.nan\n",
    "        comp_rows.append(row)\n",
    "\n",
    "    comp_df = pd.DataFrame(comp_rows) if comp_rows else pd.DataFrame({\"segment_id\": prof[\"segment_id\"]})\n",
    "    out = prof.merge(comp_df, on=\"segment_id\", how=\"left\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Anomaly detection\n",
    "# =========================\n",
    "@st.cache_data\n",
    "def compute_anomalies(d: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = d.copy()\n",
    "\n",
    "    # 1) IT spend relative to size (log regression residual)\n",
    "    y_col = \"it_spend\" if \"it_spend\" in out.columns else None\n",
    "    x_cols = [c for c in [\"employees_total\", \"revenue_usd\"] if c in out.columns]\n",
    "    if y_col and x_cols:\n",
    "        y = safe_log1p(out[y_col])\n",
    "        X = [np.ones(len(out))]\n",
    "        for c in x_cols:\n",
    "            X.append(safe_log1p(out[c]))\n",
    "        X = np.vstack(X).T\n",
    "\n",
    "        mask = np.isfinite(y.values)\n",
    "        for j in range(X.shape[1]):\n",
    "            mask = mask & np.isfinite(X[:, j])\n",
    "\n",
    "        if mask.sum() >= 50:\n",
    "            Xf = X[mask]\n",
    "            yf = y.values[mask]\n",
    "            beta, _, _, _ = np.linalg.lstsq(Xf, yf, rcond=None)\n",
    "            yhat = X @ beta\n",
    "            resid = y.values - yhat\n",
    "            out[\"it_residual\"] = resid\n",
    "            out[\"it_residual_z\"] = robust_z(pd.Series(resid, index=out.index))\n",
    "        else:\n",
    "            out[\"it_residual\"] = np.nan\n",
    "            out[\"it_residual_z\"] = np.nan\n",
    "    else:\n",
    "        out[\"it_residual\"] = np.nan\n",
    "        out[\"it_residual_z\"] = np.nan\n",
    "\n",
    "    # 2) Large subsidiaries\n",
    "    out[\"is_subsidiary_type\"] = out[\"structure_tier\"].astype(str).isin([\"subsidiary\", \"subsidiary_like\"]) if \"structure_tier\" in out.columns else False\n",
    "    out[\"subsidiary_emp_pct\"] = np.nan\n",
    "    if \"employees_total\" in out.columns:\n",
    "        subs = out[out[\"is_subsidiary_type\"] == True]\n",
    "        if len(subs) >= 20:\n",
    "            subs_emp = pd.to_numeric(subs[\"employees_total\"], errors=\"coerce\")\n",
    "            subs_emp_rank = subs_emp.rank(pct=True) * 100.0\n",
    "            out.loc[subs.index, \"subsidiary_emp_pct\"] = subs_emp_rank\n",
    "\n",
    "    # 3) Atypical device/server distributions (robust z within segment)\n",
    "    ratio_cols = [c for c in [\"server_to_device_ratio\", \"laptop_to_device_ratio\", \"desktop_to_device_ratio\"] if c in out.columns]\n",
    "    for c in ratio_cols:\n",
    "        out[c + \"_seg_z\"] = out.groupby(\"segment_id\")[c].transform(lambda s: robust_z(s))\n",
    "\n",
    "    # 4) Large corporate families\n",
    "    if \"corporate_family_members\" in out.columns:\n",
    "        fam = pd.to_numeric(out[\"corporate_family_members\"], errors=\"coerce\")\n",
    "        out[\"family_pct\"] = fam.rank(pct=True) * 100.0\n",
    "    else:\n",
    "        out[\"family_pct\"] = np.nan\n",
    "\n",
    "    # Flags + severity\n",
    "    severity = np.zeros(len(out), dtype=float)\n",
    "\n",
    "    out[\"flag_it_high_relative\"] = (pd.to_numeric(out[\"it_residual_z\"], errors=\"coerce\") >= 2.5).fillna(False)\n",
    "    out[\"flag_it_low_relative\"] = (pd.to_numeric(out[\"it_residual_z\"], errors=\"coerce\") <= -2.5).fillna(False)\n",
    "    severity += np.where(out[\"flag_it_high_relative\"], 2.0, 0.0)\n",
    "    severity += np.where(out[\"flag_it_low_relative\"], 2.0, 0.0)\n",
    "\n",
    "    out[\"flag_large_subsidiary\"] = ((pd.to_numeric(out[\"subsidiary_emp_pct\"], errors=\"coerce\") >= 95) & (out[\"is_subsidiary_type\"] == True)).fillna(False)\n",
    "    severity += np.where(out[\"flag_large_subsidiary\"], 1.5, 0.0)\n",
    "\n",
    "    for c in ratio_cols:\n",
    "        flagname = \"flag_\" + c.replace(\"_ratio\", \"\") + \"_atypical\"\n",
    "        out[flagname] = (pd.to_numeric(out[c + \"_seg_z\"], errors=\"coerce\").abs() >= 3.0).fillna(False)\n",
    "        severity += np.where(out[flagname], 1.0, 0.0)\n",
    "\n",
    "    out[\"flag_large_family\"] = (pd.to_numeric(out[\"family_pct\"], errors=\"coerce\") >= 95).fillna(False)\n",
    "    severity += np.where(out[\"flag_large_family\"], 1.0, 0.0)\n",
    "\n",
    "    out[\"anomaly_severity\"] = severity\n",
    "\n",
    "    def explain_row(r):\n",
    "        bullets = []\n",
    "        if r.get(\"flag_it_high_relative\", False):\n",
    "            z = r.get(\"it_residual_z\", np.nan)\n",
    "            bullets.append(f\"IT spend high relative to size (robust z={z:.2f})\")\n",
    "        if r.get(\"flag_it_low_relative\", False):\n",
    "            z = r.get(\"it_residual_z\", np.nan)\n",
    "            bullets.append(f\"IT spend low relative to size (robust z={z:.2f})\")\n",
    "        if r.get(\"flag_large_subsidiary\", False):\n",
    "            p = r.get(\"subsidiary_emp_pct\", np.nan)\n",
    "            bullets.append(f\"Subsidiary type but very large among subsidiaries (pct={p:.0f})\")\n",
    "        if r.get(\"flag_server_to_device_atypical\", False):\n",
    "            z = r.get(\"server_to_device_ratio_seg_z\", np.nan)\n",
    "            bullets.append(f\"Server share atypical within segment (robust z={z:.2f})\")\n",
    "        if r.get(\"flag_laptop_to_device_atypical\", False):\n",
    "            z = r.get(\"laptop_to_device_ratio_seg_z\", np.nan)\n",
    "            bullets.append(f\"Laptop share atypical within segment (robust z={z:.2f})\")\n",
    "        if r.get(\"flag_desktop_to_device_atypical\", False):\n",
    "            z = r.get(\"desktop_to_device_ratio_seg_z\", np.nan)\n",
    "            bullets.append(f\"Desktop share atypical within segment (robust z={z:.2f})\")\n",
    "        if r.get(\"flag_large_family\", False):\n",
    "            p = r.get(\"family_pct\", np.nan)\n",
    "            bullets.append(f\"Corporate family unusually large (pct={p:.0f})\")\n",
    "        return \"; \".join(bullets) if bullets else \"\"\n",
    "\n",
    "    out[\"anomaly_explanation\"] = out.apply(explain_row, axis=1)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Compute profiles/anomalies safely (prevents NameError)\n",
    "# =========================\n",
    "try:\n",
    "    profiles = build_segment_profiles(filtered)\n",
    "except Exception as e:\n",
    "    profiles = pd.DataFrame()\n",
    "    st.error(f\"Segment profiling failed: {e}\")\n",
    "\n",
    "try:\n",
    "    an_df = compute_anomalies(filtered)\n",
    "except Exception as e:\n",
    "    an_df = filtered.copy()\n",
    "    an_df[\"anomaly_severity\"] = 0.0\n",
    "    an_df[\"anomaly_explanation\"] = \"\"\n",
    "    st.error(f\"Anomaly detection failed: {e}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Benchmark helpers\n",
    "# =========================\n",
    "def compute_company_percentiles(d: pd.DataFrame, row_idx: int) -> pd.DataFrame:\n",
    "    row = d.loc[row_idx]\n",
    "    seg_id = int(row[\"segment_id\"])\n",
    "    peers = d[d[\"segment_id\"] == seg_id].copy()\n",
    "\n",
    "    metrics = [c for c in [\n",
    "        \"employees_total\", \"revenue_usd\", \"it_spend\", \"it_budget\",\n",
    "        \"device_total\", \"it_spend_to_revenue\", \"it_spend_per_employee\",\n",
    "        \"corporate_family_members\",\n",
    "        \"server_to_device_ratio\", \"laptop_to_device_ratio\", \"desktop_to_device_ratio\"\n",
    "    ] if c in d.columns]\n",
    "\n",
    "    records = []\n",
    "    for m in metrics:\n",
    "        v = pd.to_numeric(pd.Series([row[m]]), errors=\"coerce\").iloc[0]\n",
    "        if not np.isfinite(v):\n",
    "            continue\n",
    "        pct = percentile_within(peers[m], v)\n",
    "        med = float(np.nanmedian(pd.to_numeric(peers[m], errors=\"coerce\"))) if peers[m].notna().any() else np.nan\n",
    "        records.append({\n",
    "            \"metric\": m,\n",
    "            \"company_value\": float(v),\n",
    "            \"segment_median\": med,\n",
    "            \"percentile_in_segment\": pct\n",
    "        })\n",
    "    if not records:\n",
    "        return pd.DataFrame(columns=[\"metric\", \"company_value\", \"segment_median\", \"percentile_in_segment\"])\n",
    "    return pd.DataFrame(records).sort_values(\"percentile_in_segment\", ascending=False)\n",
    "\n",
    "\n",
    "def nearest_peers(d: pd.DataFrame, row_idx: int, k: int = 10) -> pd.DataFrame:\n",
    "    row = d.loc[row_idx]\n",
    "    seg_id = int(row[\"segment_id\"])\n",
    "    peers = d[d[\"segment_id\"] == seg_id].copy()\n",
    "\n",
    "    if len(peers) <= 1:\n",
    "        return peers.head(0)\n",
    "\n",
    "    feat_cols = [c for c in [\"employees_total\", \"revenue_usd\", \"it_spend\", \"device_total\", \"corporate_family_members\"] if c in d.columns]\n",
    "    if not feat_cols:\n",
    "        return peers.head(0)\n",
    "\n",
    "    X = peers[feat_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X = np.log1p(X)\n",
    "    mu = np.nanmean(X.values, axis=0)\n",
    "    sd = np.nanstd(X.values, axis=0)\n",
    "    sd = np.where(sd == 0, 1.0, sd)\n",
    "    Z = (X.values - mu) / sd\n",
    "\n",
    "    x0 = np.log1p(pd.to_numeric(row[feat_cols], errors=\"coerce\")).values.astype(float)\n",
    "    z0 = (x0 - mu) / sd\n",
    "\n",
    "    dist = np.nanmean((Z - z0) ** 2, axis=1) ** 0.5\n",
    "    peers = peers.assign(peer_distance=dist).sort_values(\"peer_distance\", ascending=True)\n",
    "    peers = peers[peers.index != row_idx]\n",
    "\n",
    "    cols_show = [\"display_name\", \"peer_distance\"]\n",
    "    for c in [\"company_sites\", \"country\", \"entity_type\", \"employees_total\", \"revenue_usd\", \"it_spend\", \"device_total\", \"segment_description\"]:\n",
    "        if c in peers.columns and c not in cols_show:\n",
    "            cols_show.append(c)\n",
    "    return peers[cols_show].head(k)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Tabs\n",
    "# =========================\n",
    "tab_overview, tab_segments, tab_company, tab_risk, tab_usecases = st.tabs(\n",
    "    [\"Overview\", \"Segments\", \"Company benchmarking\", \"Risks and anomalies\", \"Insights for buyers\"]\n",
    ")\n",
    "\n",
    "# ---------- Overview ----------\n",
    "with tab_overview:\n",
    "    st.subheader(\"Overview\")\n",
    "\n",
    "    c1, c2, c3, c4 = st.columns(4)\n",
    "    c1.metric(\"Companies (filtered)\", f\"{len(filtered):,}\")\n",
    "    c2.metric(\"Segments\", f\"{filtered['segment_id'].nunique():,}\")\n",
    "    flagged = int((an_df.get(\"anomaly_severity\", 0) > 0).sum()) if \"anomaly_severity\" in an_df.columns else 0\n",
    "    c3.metric(\"Flagged companies\", f\"{flagged:,}\")\n",
    "    c4.metric(\"Countries\", f\"{filtered['country'].nunique(dropna=True) if 'country' in filtered.columns else 0:,}\")\n",
    "\n",
    "    with st.expander(\"Data health\", expanded=False):\n",
    "        st.write(\n",
    "            f\"Dedup stats: {raw_df.attrs.get('dedup_before', 'NA')} -> \"\n",
    "            f\"{raw_df.attrs.get('dedup_after_exact', 'NA')} -> \"\n",
    "            f\"{raw_df.attrs.get('dedup_after_key', 'NA')}\"\n",
    "        )\n",
    "        key_cols = [c for c in [\"company_sites\", \"sic_code\", \"8_digit_sic_code\", \"employees_total\", \"revenue_usd\", \"it_spend\", \"device_total\", \"corporate_family_members\", \"country\", \"entity_type\"] if c in filtered.columns]\n",
    "        if key_cols:\n",
    "            miss = (filtered[key_cols].isna().mean() * 100).round(1).sort_values(ascending=False)\n",
    "            miss_df = miss.reset_index()\n",
    "            miss_df.columns = [\"column\", \"missing_percent\"]\n",
    "            st.dataframe(miss_df, use_container_width=True)\n",
    "\n",
    "    st.markdown(\"Top segments\")\n",
    "    seg_counts = (\n",
    "        filtered.groupby([\"segment_id\", \"segment_description\"], dropna=False)\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "    )\n",
    "    st.dataframe(seg_counts.head(30), use_container_width=True)\n",
    "\n",
    "    if \"country\" in filtered.columns:\n",
    "        st.markdown(\"Top countries\")\n",
    "        ctab = filtered[\"country\"].fillna(\"Unknown\").astype(str).value_counts().head(20).reset_index()\n",
    "        ctab.columns = [\"country\", \"count\"]\n",
    "        st.dataframe(ctab, use_container_width=True)\n",
    "\n",
    "# ---------- Segments ----------\n",
    "with tab_segments:\n",
    "    st.subheader(\"Segment profiles\")\n",
    "    st.caption(\"Profiles summarise typical values (median) and dominant categories per segment.\")\n",
    "\n",
    "    if profiles is None or profiles.empty:\n",
    "        st.warning(\"No segment profiles available (profiling failed or no data after filters).\")\n",
    "    else:\n",
    "        # Show human description early\n",
    "        show_cols = [\"segment_id\", \"segment_description\", \"count\", \"segment_label\"]\n",
    "        for c in profiles.columns:\n",
    "            if c not in show_cols:\n",
    "                show_cols.append(c)\n",
    "        show_cols = [c for c in show_cols if c in profiles.columns]\n",
    "\n",
    "        st.dataframe(profiles[show_cols].head(100), use_container_width=True, height=420)\n",
    "\n",
    "        seg_ids = profiles[\"segment_id\"].astype(int).tolist()\n",
    "        selected_seg = st.selectbox(\"Select a segment to deep dive\", options=seg_ids, format_func=lambda x: f\"S{int(x)}\")\n",
    "\n",
    "        seg_row = profiles[profiles[\"segment_id\"] == selected_seg].head(1)\n",
    "        seg_label = seg_row[\"segment_label\"].iloc[0] if len(seg_row) else \"Unknown\"\n",
    "        seg_desc = seg_row[\"segment_description\"].iloc[0] if len(seg_row) else \"\"\n",
    "        seg_df = filtered[filtered[\"segment_id\"] == selected_seg].copy()\n",
    "\n",
    "        st.write(f\"Selected segment: S{int(selected_seg)}\")\n",
    "        if seg_desc:\n",
    "            st.caption(seg_desc)\n",
    "\n",
    "        with st.expander(\"Raw segment label (technical)\", expanded=False):\n",
    "            st.code(str(seg_label))\n",
    "\n",
    "        st.markdown(\"Typical values (median) and top composition fields\")\n",
    "        st.dataframe(seg_row, use_container_width=True)\n",
    "\n",
    "        cA, cB, cC = st.columns(3)\n",
    "        with cA:\n",
    "            if \"country\" in seg_df.columns:\n",
    "                st.write(\"Country composition (top 10)\")\n",
    "                t = seg_df[\"country\"].fillna(\"Unknown\").astype(str).value_counts().head(10).reset_index()\n",
    "                t.columns = [\"country\", \"count\"]\n",
    "                st.dataframe(t, use_container_width=True, height=260)\n",
    "\n",
    "        with cB:\n",
    "            if \"entity_type\" in seg_df.columns:\n",
    "                st.write(\"Entity type composition (top 10)\")\n",
    "                t = seg_df[\"entity_type\"].fillna(\"Unknown\").astype(str).value_counts().head(10).reset_index()\n",
    "                t.columns = [\"entity_type\", \"count\"]\n",
    "                st.dataframe(t, use_container_width=True, height=260)\n",
    "\n",
    "        with cC:\n",
    "            ind_col = \"8_digit_sic_description\" if \"8_digit_sic_description\" in seg_df.columns else (\"sic_description\" if \"sic_description\" in seg_df.columns else None)\n",
    "            if ind_col:\n",
    "                st.write(\"Industry composition (top 10)\")\n",
    "                t = seg_df[ind_col].fillna(\"Unknown\").astype(str).value_counts().head(10).reset_index()\n",
    "                t.columns = [\"industry\", \"count\"]\n",
    "                st.dataframe(t, use_container_width=True, height=260)\n",
    "\n",
    "# ---------- Company benchmarking ----------\n",
    "with tab_company:\n",
    "    st.subheader(\"Company benchmarking within segment peers\")\n",
    "\n",
    "    companies = sorted(filtered[\"display_name\"].fillna(\"UNKNOWN\").astype(str).unique())\n",
    "    if not companies:\n",
    "        st.info(\"No companies found in current filtered view.\")\n",
    "    else:\n",
    "        company = st.selectbox(\"Select a company\", companies)\n",
    "        row_df = filtered[filtered[\"display_name\"].astype(str) == str(company)]\n",
    "        if len(row_df) == 0:\n",
    "            st.warning(\"Company not found after filters.\")\n",
    "        else:\n",
    "            row_idx = row_df.index[0]\n",
    "            row = filtered.loc[row_idx]\n",
    "            seg_id = int(row[\"segment_id\"])\n",
    "\n",
    "            st.write(f\"Segment: S{seg_id}\")\n",
    "            seg_desc = str(row.get(\"segment_description\", \"\")).strip()\n",
    "            if seg_desc:\n",
    "                st.caption(seg_desc)\n",
    "\n",
    "            with st.expander(\"Raw segment label (technical)\", expanded=False):\n",
    "                st.code(str(row.get(\"segment_label\", \"Unknown\")))\n",
    "\n",
    "            show_cols = [\n",
    "                \"company_sites\", \"display_name\", \"country\", \"entity_type\",\n",
    "                \"sic_description\", \"8_digit_sic_description\",\n",
    "                \"employees_total\", \"revenue_usd\",\n",
    "                \"it_spend\", \"it_budget\",\n",
    "                \"device_total\", \"corporate_family_members\",\n",
    "                \"structure_tier\",\n",
    "                \"website\", \"phone_number\", \"address_line_1\"\n",
    "            ]\n",
    "            show_cols = [c for c in show_cols if c in filtered.columns]\n",
    "            st.markdown(\"Company record (selected columns)\")\n",
    "            st.dataframe(filtered.loc[[row_idx], show_cols], use_container_width=True)\n",
    "\n",
    "            st.markdown(\"Benchmark vs segment peers (percentiles and medians)\")\n",
    "            st.dataframe(compute_company_percentiles(filtered, row_idx), use_container_width=True)\n",
    "\n",
    "            st.markdown(\"Nearest peers in the same segment\")\n",
    "            st.dataframe(nearest_peers(filtered, row_idx, k=10), use_container_width=True)\n",
    "\n",
    "            st.markdown(\"Auto insights (data grounded)\")\n",
    "            insights = []\n",
    "            bench = compute_company_percentiles(filtered, row_idx)\n",
    "\n",
    "            if not bench.empty:\n",
    "                def get_pct(metric):\n",
    "                    s = bench[bench[\"metric\"] == metric]\n",
    "                    return float(s[\"percentile_in_segment\"].iloc[0]) if len(s) else np.nan\n",
    "\n",
    "                p_it = get_pct(\"it_spend\")\n",
    "                if np.isfinite(p_it):\n",
    "                    if p_it >= 85:\n",
    "                        insights.append(f\"IT spend is higher than {p_it:.0f}% of peers in this segment.\")\n",
    "                    elif p_it <= 15:\n",
    "                        insights.append(f\"IT spend is lower than {100 - p_it:.0f}% of peers in this segment.\")\n",
    "\n",
    "                p_emp = get_pct(\"employees_total\")\n",
    "                if np.isfinite(p_emp) and str(row.get(\"structure_tier\", \"\")) in {\"subsidiary\", \"subsidiary_like\"} and p_emp >= 90:\n",
    "                    insights.append(f\"This company is subsidiary type but large for its peer group (employees around {p_emp:.0f}th percentile).\")\n",
    "\n",
    "                p_fam = get_pct(\"corporate_family_members\")\n",
    "                if np.isfinite(p_fam) and p_fam >= 95:\n",
    "                    insights.append(f\"Corporate family size is unusually large (around {p_fam:.0f}th percentile in segment).\")\n",
    "\n",
    "            if row_idx in an_df.index and \"anomaly_explanation\" in an_df.columns:\n",
    "                expl = str(an_df.loc[row_idx].get(\"anomaly_explanation\", \"\")).strip()\n",
    "                if expl:\n",
    "                    insights.append(f\"Anomaly flags: {expl}\")\n",
    "\n",
    "            if insights:\n",
    "                for s in insights:\n",
    "                    st.write(\" \" + s)\n",
    "            else:\n",
    "                st.caption(\"No strong signals detected from the current metrics and thresholds.\")\n",
    "\n",
    "# ---------- Risks and anomalies ----------\n",
    "with tab_risk:\n",
    "    st.subheader(\"Risks and anomalies\")\n",
    "    st.caption(\"Flagged companies are surfaced using robust statistics with evidence-based explanations.\")\n",
    "\n",
    "    flagged_only = st.toggle(\"Show flagged companies only\", value=True)\n",
    "    view = an_df.copy()\n",
    "    if \"anomaly_severity\" not in view.columns:\n",
    "        view[\"anomaly_severity\"] = 0.0\n",
    "    if \"anomaly_explanation\" not in view.columns:\n",
    "        view[\"anomaly_explanation\"] = \"\"\n",
    "\n",
    "    if flagged_only:\n",
    "        view = view[view[\"anomaly_severity\"] > 0]\n",
    "\n",
    "    view = view.sort_values([\"anomaly_severity\"], ascending=False)\n",
    "\n",
    "    cols = [\"company_sites\", \"display_name\", \"country\", \"entity_type\", \"segment_description\", \"anomaly_severity\", \"anomaly_explanation\"]\n",
    "    cols = [c for c in cols if c in view.columns]\n",
    "    for c in [\"employees_total\", \"revenue_usd\", \"it_spend\", \"device_total\", \"corporate_family_members\", \"it_spend_to_revenue\", \"server_to_device_ratio\"]:\n",
    "        if c in view.columns:\n",
    "            cols.append(c)\n",
    "\n",
    "    st.dataframe(view[cols].head(300), use_container_width=True, height=520)\n",
    "    st.download_button(\n",
    "        \"Download risk list as CSV\",\n",
    "        data=view[cols].to_csv(index=False).encode(\"utf-8\"),\n",
    "        file_name=\"risk_flags.csv\",\n",
    "        mime=\"text/csv\"\n",
    "    )\n",
    "\n",
    "# ---------- Buyer use cases ----------\n",
    "with tab_usecases:\n",
    "\n",
    "    st.markdown(\"### 1) Dataset company list\")\n",
    "    lead_cols = [\n",
    "        \"company_sites\", \"display_name\", \"country\", \"state\", \"city\",\n",
    "        \"segment_description\",\n",
    "        \"sic_description\", \"8_digit_sic_description\",\n",
    "        \"employees_total\", \"revenue_usd\", \"it_spend\", \"device_total\",\n",
    "        \"website\", \"phone_number\"\n",
    "    ]\n",
    "    lead_cols = [c for c in lead_cols if c in filtered.columns]\n",
    "    st.dataframe(filtered[lead_cols].head(200), use_container_width=True)\n",
    "    st.download_button(\n",
    "        \"Download leads as CSV\",\n",
    "        data=filtered[lead_cols].to_csv(index=False).encode(\"utf-8\"),\n",
    "        file_name=\"leads.csv\",\n",
    "        mime=\"text/csv\"\n",
    "    )\n",
    "\n",
    "    st.markdown(\"### 2) Competitive benchmarking\")\n",
    "    companies2 = sorted(filtered[\"display_name\"].fillna(\"UNKNOWN\").astype(str).unique())\n",
    "    if companies2:\n",
    "        comp2 = st.selectbox(\"Select a company for benchmarking\", companies2, key=\"usecase_company\")\n",
    "        row_df2 = filtered[filtered[\"display_name\"].astype(str) == str(comp2)]\n",
    "        if len(row_df2):\n",
    "            idx2 = row_df2.index[0]\n",
    "            st.dataframe(compute_company_percentiles(filtered, idx2), use_container_width=True)\n",
    "            st.dataframe(nearest_peers(filtered, idx2, k=15), use_container_width=True)\n",
    "\n",
    "    st.markdown(\"### 3) Risk assessment and compliance screening\")\n",
    "    risk_view = an_df.copy()\n",
    "    if \"anomaly_severity\" in risk_view.columns:\n",
    "        risk_view = risk_view[risk_view[\"anomaly_severity\"] > 0].sort_values(\"anomaly_severity\", ascending=False)\n",
    "    risk_cols = [c for c in [\"company_sites\", \"display_name\", \"country\", \"segment_description\", \"anomaly_severity\", \"anomaly_explanation\"] if c in risk_view.columns]\n",
    "    st.dataframe(risk_view[risk_cols].head(200), use_container_width=True)\n",
    "    st.download_button(\n",
    "        \"Download screening list as CSV\",\n",
    "        data=risk_view[risk_cols].to_csv(index=False).encode(\"utf-8\"),\n",
    "        file_name=\"screening.csv\",\n",
    "        mime=\"text/csv\"\n",
    "    )\n",
    "\n",
    "    st.markdown(\"### 4) Technology investment analysis\")\n",
    "    if profiles is None or profiles.empty:\n",
    "        st.info(\"Segment profiles not available, cannot rank segments by IT intensity.\")\n",
    "    else:\n",
    "        rank_metric_opts = [c for c in [\"it_spend_to_revenue\", \"it_spend_per_employee\", \"it_spend\", \"device_total\"] if c in profiles.columns]\n",
    "        if rank_metric_opts:\n",
    "            rank_metric = st.selectbox(\"Rank segments by\", options=rank_metric_opts, index=0)\n",
    "            seg_rank = profiles.sort_values(rank_metric, ascending=False)\n",
    "            seg_it_cols = [\"segment_id\", \"segment_description\", \"count\"] + [c for c in rank_metric_opts if c in seg_rank.columns]\n",
    "            st.dataframe(seg_rank[seg_it_cols].head(100), use_container_width=True)\n",
    "        else:\n",
    "            st.info(\"No IT intensity metrics found in profiles.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Optional AI Assistant (Sidebar)\n",
    "# =========================\n",
    "def get_llm_client():\n",
    "    if not HF_AVAILABLE:\n",
    "        return None\n",
    "    token = None\n",
    "    try:\n",
    "        token = st.secrets.get(\"HF_TOKEN\", None)\n",
    "    except Exception:\n",
    "        token = None\n",
    "    if not token:\n",
    "        return None\n",
    "    try:\n",
    "        return InferenceClient(model=\"Qwen/Qwen2.5-72B-Instruct\", token=token)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_dataframe_context(df_in: pd.DataFrame, max_rows=8) -> str:\n",
    "    if df_in is None or df_in.empty:\n",
    "        return \"The dataset view is empty.\"\n",
    "    row_count = len(df_in)\n",
    "    col_names = \", \".join(df_in.columns.tolist())\n",
    "    preview = df_in.head(max_rows).to_string(index=False)\n",
    "    return (\n",
    "        f\"Dataset Summary:\\n\"\n",
    "        f\"Total Rows in current view: {row_count}\\n\"\n",
    "        f\"Columns: {col_names}\\n\\n\"\n",
    "        f\"Data Preview (first {max_rows} rows):\\n{preview}\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "with st.sidebar:\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"AI Data Assistant\")\n",
    "\n",
    "    enable_assistant = st.toggle(\"Enable assistant\", value=False, disabled=(not HF_AVAILABLE))\n",
    "    llm_client = get_llm_client() if enable_assistant else None\n",
    "\n",
    "    if enable_assistant and llm_client is None:\n",
    "        st.caption(\"Assistant unavailable. Add HF_TOKEN in .streamlit/secrets.toml or disable.\")\n",
    "\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "    for msg in st.session_state.messages:\n",
    "        with st.chat_message(msg[\"role\"]):\n",
    "            st.markdown(msg[\"content\"])\n",
    "\n",
    "    if enable_assistant and llm_client is not None:\n",
    "        if prompt := st.chat_input(\"Ask about the filtered data...\"):\n",
    "            st.chat_message(\"user\").markdown(prompt)\n",
    "            st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "            context = get_dataframe_context(filtered, max_rows=8)\n",
    "            seg_summary = filtered.groupby(\"segment_description\").size().sort_values(ascending=False).head(10).to_string()\n",
    "\n",
    "            full_prompt = (\n",
    "                \"You are a careful data analyst assistant.\\n\"\n",
    "                \"Answer ONLY from the provided context.\\n\"\n",
    "                \"If context is insufficient, say what is missing.\\n\\n\"\n",
    "                f\"CONTEXT:\\n{context}\\n\"\n",
    "                f\"Top 10 segments by count:\\n{seg_summary}\\n\\n\"\n",
    "                f\"USER QUESTION:\\n{prompt}\\n\"\n",
    "            )\n",
    "\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                try:\n",
    "                    resp = llm_client.chat_completion(\n",
    "                        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "                        max_tokens=220,\n",
    "                        stream=False\n",
    "                    )\n",
    "                    txt = \"\"\n",
    "                    if hasattr(resp, \"choices\") and resp.choices:\n",
    "                        txt = resp.choices[0].message.content\n",
    "                    txt = (txt or \"\").strip()\n",
    "                    st.markdown(txt if txt else \"No response returned.\")\n",
    "                    st.session_state.messages.append({\"role\": \"assistant\", \"content\": txt if txt else \"No response returned.\"})\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Error communicating with API: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0f91773-c8bf-4e2d-b225-e118c6fcc497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open: http://localhost:8502  |  PID: 35158\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "p = subprocess.Popen([sys.executable, \"-m\", \"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8502\", \"--server.headless\", \"true\"])\n",
    "print(\"Open: http://localhost:8502  |  PID:\", p.pid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5383f311-af21-4ed5-83d1-2f00a16e2b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Port 8502 is busy. You might need to restart the kernel to free it.\n",
      " Launching Streamlit... (Please wait, this can take 5-10 seconds)\n",
      " App is online! Opening browser...\n",
      "--- App Logs (Stop cell to exit) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-23 17:45:12.255 Port 8502 is not available\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import webbrowser\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "PORT = 8502\n",
    "SCRIPT_NAME = \"app.py\"\n",
    "\n",
    "def is_port_in_use(port):\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        return s.connect_ex(('localhost', port)) == 0\n",
    "\n",
    "# 1. Kill any previous instance (Just in case)\n",
    "if is_port_in_use(PORT):\n",
    "    print(f\" Port {PORT} is busy. You might need to restart the kernel to free it.\")\n",
    "    # Optional: logic to kill the specific process could go here, \n",
    "    # but restarting the kernel is safer for notebooks.\n",
    "\n",
    "# 2. Start Streamlit in the background\n",
    "print(f\" Launching Streamlit... (Please wait, this can take 5-10 seconds)\")\n",
    "process = subprocess.Popen(\n",
    "    [sys.executable, \"-m\", \"streamlit\", \"run\", SCRIPT_NAME, \n",
    "     \"--server.port\", str(PORT), \n",
    "     \"--server.headless\", \"true\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# 3. \"Smart Wait\" loop\n",
    "server_url = f\"http://localhost:{PORT}\"\n",
    "app_started = False\n",
    "start_time = time.time()\n",
    "\n",
    "while time.time() - start_time < 30:  # Timeout after 30 seconds\n",
    "    try:\n",
    "        # Ping the server to see if it's awake\n",
    "        response = requests.get(f\"{server_url}/_stcore/health\")\n",
    "        if response.status_code == 200:\n",
    "            app_started = True\n",
    "            break\n",
    "    except requests.ConnectionError:\n",
    "        # Server not ready yet, wait and retry\n",
    "        time.sleep(1)\n",
    "        continue\n",
    "\n",
    "# 4. Final Result\n",
    "if app_started:\n",
    "    print(f\" App is online! Opening browser...\")\n",
    "    webbrowser.open(server_url)\n",
    "    \n",
    "    # Optional: Print logs to help debug if it crashes later\n",
    "    # Warning: This loop blocks the cell. Stop the cell to stop printing.\n",
    "    print(\"--- App Logs (Stop cell to exit) ---\")\n",
    "    for line in iter(process.stdout.readline, ''):\n",
    "        print(line, end=\"\")\n",
    "else:\n",
    "    print(\" App failed to start within 30 seconds.\")\n",
    "    # Print the error to see WHY it failed\n",
    "    print(\"--- Error Logs ---\")\n",
    "    print(process.stderr.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba4a1ad-6102-441d-8fd1-0c41e948b1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
