{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "N_smoqVgjUOW",
   "metadata": {
    "id": "N_smoqVgjUOW"
   },
   "source": [
    "# **NUS DATHATON 2026**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cwbDfwIyjhXv",
   "metadata": {
    "id": "cwbDfwIyjhXv"
   },
   "source": [
    "***2 Sons 2 Daughters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d996f3c-845c-4bcf-9039-88adae5124f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.53.0)\n",
      "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<7,>=5.5 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.2.4)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (8.2.1)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (25.0)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (2.3.3)\n",
      "Requirement already satisfied: pillow<13,>=7.1.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (12.1.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.33.4)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (23.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (2.32.5)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (4.15.0)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (3.1.46)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from streamlit) (6.5.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.26.0)\n",
      "Requirement already satisfied: narwhals>=1.27.1 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2026.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (2026.1.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: shellingham in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\yee hui\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer-slim->huggingface_hub) (8.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit\n",
    "!pip install openpyxl\n",
    "!pip install matplotlib\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c26ad86b-c8c2-46bf-bba2-13b86d196c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file created at: C:\\Users\\Yee Hui/.streamlit\\credentials.toml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the streamlit configuration directory\n",
    "streamlit_config_dir = os.path.expanduser(\"~/.streamlit\")\n",
    "os.makedirs(streamlit_config_dir, exist_ok=True)\n",
    "\n",
    "# Create the credentials.toml file with a blank email\n",
    "config_path = os.path.join(streamlit_config_dir, \"credentials.toml\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    f.write('[general]\\nemail = \"\"')\n",
    "\n",
    "print(f\"Configuration file created at: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "071019bc-6d46-4e51-ae24-9ef5da9031f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\"\"\"\n",
    "Streamlit app (clean + segment + explore) for Champions Group dataset.\n",
    "\n",
    "Fixes in this version:\n",
    "- NO pandas \"string\" dtype anywhere (uses plain object/str), so it won't crash with:\n",
    "  TypeError: data type 'string' not understood\n",
    "- Robust bucket parsing for device/server fields (eg '1 to 10', '1,001 to 5,000', '100000+')\n",
    "- Content normalisation (country casing, missing tokens, phone formatting)\n",
    "- Keeps only columns needed for the 5 attribute groups + a few UI display fields\n",
    "- Adds rule-based segments (interpretable)\n",
    "\n",
    "Run:\n",
    "  streamlit run app.py\n",
    "\"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Initialize Client (Best practice: use st.secrets, but for now we paste directly)\n",
    "# REPLACE 'hf_xxxxxxxx' WITH YOUR ACTUAL TOKEN\n",
    "HF_TOKEN = \"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\" \n",
    "repo_id = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "llm_client = InferenceClient(model=repo_id, token=HF_TOKEN)\n",
    "\n",
    "def get_dataframe_context(df, max_rows=5):\n",
    "    \"\"\"\n",
    "    Creates a text summary of the current dataframe to send to the LLM.\n",
    "    Uses to_string() to avoid dependency on 'tabulate'.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return \"The dataset is currently empty.\"\n",
    "    \n",
    "    # metrics summary\n",
    "    row_count = len(df)\n",
    "    col_names = \", \".join(df.columns.tolist())\n",
    "    \n",
    "    # CHANGE: Use to_string instead of to_markdown\n",
    "    preview = df.head(max_rows).to_string(index=False)\n",
    "    \n",
    "    context = f\"\"\"\n",
    "    Dataset Summary:\n",
    "    - Total Rows in current view: {row_count}\n",
    "    - Columns: {col_names}\n",
    "    \n",
    "    Data Preview (First {max_rows} rows):\n",
    "    {preview}\n",
    "    \"\"\"\n",
    "    return context\n",
    "\n",
    "# -----------------------------\n",
    "# Streamlit page\n",
    "# -----------------------------\n",
    "st.set_page_config(page_title=\"Company Intelligence Explorer\", layout=\"wide\")\n",
    "st.title(\"Company Segmentation and Intelligence Explorer\")\n",
    "\n",
    "# -----------------------------\n",
    "# Upload (avoid file path issues)\n",
    "# -----------------------------\n",
    "uploaded = st.sidebar.file_uploader(\"Upload Excel (.xlsx)\", type=[\"xlsx\"])\n",
    "if uploaded is None:\n",
    "    st.info(\"Upload the dataset to begin.\")\n",
    "    st.stop()\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: column name + content cleaning\n",
    "# -----------------------------\n",
    "def to_snake(s: str) -> str:\n",
    "    \"\"\"Convert column names to snake_case-like strings.\"\"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"[^\\w]+\", \"_\", s)  # replace non-alphanumeric with _\n",
    "    return s.strip(\"_\")\n",
    "\n",
    "def normalise_missing_text(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Standardise common missing tokens into actual NA.\n",
    "    Uses plain object dtype (not pandas StringDtype).\n",
    "    \"\"\"\n",
    "    s = series.astype(object)\n",
    "    miss = {\"\", \"na\", \"n/a\", \"none\", \"null\", \"unknown\", \"nan\"}\n",
    "    out = []\n",
    "    for v in s.values:\n",
    "        if pd.isna(v):\n",
    "            out.append(pd.NA)\n",
    "            continue\n",
    "        t = str(v).strip()\n",
    "        if t.lower() in miss:\n",
    "            out.append(pd.NA)\n",
    "        else:\n",
    "            out.append(t)\n",
    "    return pd.Series(out, index=series.index, dtype=object)\n",
    "\n",
    "def clean_phone(x):\n",
    "    \"\"\"\n",
    "    Phone numbers sometimes appear as floats/scientific notation in Excel.\n",
    "    Convert to a clean digit string where possible.\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return pd.NA\n",
    "    s = str(x).strip()\n",
    "\n",
    "    # Try convert scientific notation -> int string\n",
    "    try:\n",
    "        f = float(s)\n",
    "        if np.isfinite(f):\n",
    "            return str(int(f))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Remove trailing .0\n",
    "    if re.match(r\"^\\d+\\.0$\", s):\n",
    "        return s[:-2]\n",
    "\n",
    "    return s\n",
    "\n",
    "def bucket_to_midpoint(x):\n",
    "    \"\"\"\n",
    "    Parse range buckets commonly found in device/server columns.\n",
    "    Examples:\n",
    "      '1 to 10' -> 5.5\n",
    "      '1,001 to 5,000' -> 3000.5\n",
    "      '100000+' -> 100000\n",
    "      '12' -> 12\n",
    "    Returns np.nan if not parseable.\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip().lower().replace(\",\", \"\")\n",
    "    if s in {\"\", \"na\", \"n/a\", \"none\", \"null\", \"unknown\"}:\n",
    "        return np.nan\n",
    "\n",
    "    # '100000+' style\n",
    "    m = re.match(r\"^(\\d+)\\s*\\+$\", s)\n",
    "    if m:\n",
    "        return float(m.group(1))\n",
    "\n",
    "    # 'a to b' or 'a - b' style\n",
    "    m = re.match(r\"^(\\d+)\\s*(to|-)\\s*(\\d+)$\", s)\n",
    "    if m:\n",
    "        a, b = float(m.group(1)), float(m.group(3))\n",
    "        return (a + b) / 2.0\n",
    "\n",
    "    # plain number\n",
    "    m = re.match(r\"^\\d+(\\.\\d+)?$\", s)\n",
    "    if m:\n",
    "        return float(s)\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "def safe_numeric(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert to numeric safely.\n",
    "    If most values are not numeric, try bucket_to_midpoint (useful for device bucket fields).\n",
    "    \"\"\"\n",
    "    x = pd.to_numeric(series, errors=\"coerce\")\n",
    "    # If too many NaNs after numeric conversion, attempt bucket parsing\n",
    "    if x.notna().mean() < 0.30:\n",
    "        x2 = series.map(bucket_to_midpoint)\n",
    "        if pd.Series(x2).notna().mean() > x.notna().mean():\n",
    "            return pd.to_numeric(x2, errors=\"coerce\")\n",
    "    return x\n",
    "\n",
    "def zero_to_nan(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Many columns use 0 as placeholder for missing.\n",
    "    Convert 0 -> NaN for columns where true 0 is unlikely (revenue, employees, IT spend, etc.).\n",
    "    \"\"\"\n",
    "    x = pd.to_numeric(series, errors=\"coerce\")\n",
    "    return x.mask(x == 0, np.nan)\n",
    "\n",
    "def pick_col(df: pd.DataFrame, candidates):\n",
    "    \"\"\"Find first existing column name from a list of candidates.\"\"\"\n",
    "    cols = set(df.columns)\n",
    "    for cand in candidates:\n",
    "        if cand in cols:\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# Load + Clean + Keep relevant columns\n",
    "# -----------------------------\n",
    "@st.cache_data\n",
    "def load_and_clean(file) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load Excel and apply:\n",
    "    - snake_case columns\n",
    "    - content normalisation (country casing, phone, missing tokens)\n",
    "    - numeric conversion for key fields\n",
    "    - device bucket parsing into numeric midpoints (keeping raw bucket text too)\n",
    "    - keep only columns relevant to the 5 attribute groups + UI identifiers\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file)\n",
    "    df.columns = [to_snake(c) for c in df.columns]\n",
    "\n",
    "    # ---------- General content normalisation ----------\n",
    "    for c in [\n",
    "        \"country\", \"city\", \"state\", \"state_or_province_abbreviation\",\n",
    "        \"entity_type\", \"ownership_type\", \"legal_status\",\n",
    "        \"company_status_active_inactive\", \"sic_description\", \"8_digit_sic_description\"\n",
    "    ]:\n",
    "        if c in df.columns:\n",
    "            df[c] = normalise_missing_text(df[c])\n",
    "\n",
    "    # Country: case normalisation (CHINA vs China)\n",
    "    if \"country\" in df.columns:\n",
    "        df[\"country\"] = df[\"country\"].astype(object).apply(lambda x: str(x).upper().strip() if not pd.isna(x) else pd.NA)\n",
    "\n",
    "    # Entity type: consistent casing for UI\n",
    "    if \"entity_type\" in df.columns:\n",
    "        df[\"entity_type\"] = df[\"entity_type\"].astype(object).apply(lambda x: str(x).strip().title() if not pd.isna(x) else pd.NA)\n",
    "\n",
    "    # Phone number: fix float/scientific notation\n",
    "    if \"phone_number\" in df.columns:\n",
    "        df[\"phone_number\"] = df[\"phone_number\"].apply(clean_phone)\n",
    "\n",
    "    # ---------- Numeric conversions ----------\n",
    "    placeholder_zero_cols = [\n",
    "        \"employees_total\", \"employees_single_site\",\n",
    "        \"revenue_usd\", \"market_value_usd\",\n",
    "        \"it_budget\", \"it_spend\",\n",
    "        \"corporate_family_members\"\n",
    "    ]\n",
    "    for c in placeholder_zero_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = zero_to_nan(df[c])\n",
    "\n",
    "    # Year found: sanity bounds\n",
    "    if \"year_found\" in df.columns:\n",
    "        y = pd.to_numeric(df[\"year_found\"], errors=\"coerce\")\n",
    "        df[\"year_found\"] = y.mask((y <= 1700) | (y > 2026), np.nan)\n",
    "\n",
    "    # Device/server columns: parse bucket ranges -> numeric midpoints\n",
    "    device_cols = [\n",
    "        \"no_of_pc\", \"no_of_desktops\", \"no_of_laptops\",\n",
    "        \"no_of_routers\", \"no_of_servers\", \"no_of_storage_devices\"\n",
    "    ]\n",
    "    for c in device_cols:\n",
    "        if c in df.columns:\n",
    "            # keep raw bucket text for display/debug\n",
    "            df[c + \"_bucket\"] = normalise_missing_text(df[c])\n",
    "            # numeric for modelling\n",
    "            df[c] = safe_numeric(df[c])\n",
    "\n",
    "    # device_total derived\n",
    "    dev_present = [c for c in device_cols if c in df.columns]\n",
    "    df[\"device_total\"] = df[dev_present].sum(axis=1, min_count=1) if dev_present else np.nan\n",
    "\n",
    "    # Derived metrics (safe division)\n",
    "    if \"employees_total\" in df.columns and \"revenue_usd\" in df.columns:\n",
    "        df[\"revenue_per_employee\"] = df[\"revenue_usd\"] / df[\"employees_total\"]\n",
    "    if \"it_spend\" in df.columns and \"revenue_usd\" in df.columns:\n",
    "        df[\"it_spend_to_revenue\"] = df[\"it_spend\"] / df[\"revenue_usd\"]\n",
    "\n",
    "    # ---------- Keep only relevant columns ----------\n",
    "    industry_cols = [\"sic_code\", \"sic_description\", \"8_digit_sic_code\", \"8_digit_sic_description\"]\n",
    "    size_cols = [\"employees_single_site\", \"employees_total\", \"revenue_usd\", \"market_value_usd\", \"year_found\", \"revenue_per_employee\"]\n",
    "    structure_cols = [\n",
    "        \"entity_type\", \"ownership_type\", \"corporate_family_members\",\n",
    "        \"is_headquarters\", \"is_domestic_ultimate\",\n",
    "        \"parent_company\", \"global_ultimate_company\", \"domestic_ultimate_company\"\n",
    "    ]\n",
    "    it_cols = [\"it_budget\", \"it_spend\", \"it_spend_to_revenue\", \"device_total\"] + device_cols + [c + \"_bucket\" for c in device_cols if c in df.columns]\n",
    "    geo_cols = [\"country\", \"state\", \"state_or_province_abbreviation\", \"city\", \"postal_code\", \"lattitude\", \"longitude\"]\n",
    "    ui_cols = [\n",
    "        \"duns_number\", \"company_sites\", \"website\", \"address_line_1\",\n",
    "        \"phone_number\", \"registration_number\",\n",
    "        \"company_description\", \"company_status_active_inactive\", \"legal_status\"\n",
    "    ]\n",
    "\n",
    "    keep = []\n",
    "    for grp in [industry_cols, size_cols, structure_cols, it_cols, geo_cols, ui_cols]:\n",
    "        keep += [c for c in grp if c in df.columns]\n",
    "    keep = list(dict.fromkeys(keep))  # de-dupe preserving order\n",
    "\n",
    "    df = df[keep].copy()\n",
    "    return df\n",
    "\n",
    "df = load_and_clean(uploaded)\n",
    "\n",
    "# -----------------------------\n",
    "# Segmentation (rule-based, interpretable) - NO \"string\" dtype used\n",
    "# -----------------------------\n",
    "@st.cache_data\n",
    "def add_rule_segments(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add interpretable segments based on:\n",
    "    - Industry: SIC prefix (2 digits)\n",
    "    - Size: employee/revenue tiers\n",
    "    - Corporate structure: HQ / domestic ultimate / subsidiary / branch / etc.\n",
    "    - IT footprint: IT spend tiers + device tiers\n",
    "    - Geo: country\n",
    "\n",
    "    All text columns are handled as plain object/str to avoid pandas StringDtype issues.\n",
    "    \"\"\"\n",
    "    df = df_in.copy()\n",
    "\n",
    "    # ---------- Industry bucket ----------\n",
    "    sic_col = \"8_digit_sic_code\" if \"8_digit_sic_code\" in df.columns else (\"sic_code\" if \"sic_code\" in df.columns else None)\n",
    "\n",
    "    def digits_only(x):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        s = re.sub(r\"\\D+\", \"\", str(x))\n",
    "        return s if s else np.nan\n",
    "\n",
    "    def sic_prefix(x, n=2):\n",
    "        s = digits_only(x)\n",
    "        if pd.isna(s):\n",
    "            return \"Unknown\"\n",
    "        return s[:n].zfill(n)\n",
    "\n",
    "    df[\"sic_2digit\"] = df[sic_col].map(lambda x: sic_prefix(x, 2)) if sic_col else \"Unknown\"\n",
    "\n",
    "    # ---------- Size tiers ----------\n",
    "    def safe_qcut(series, q=4, labels=None):\n",
    "        s = pd.to_numeric(series, errors=\"coerce\")\n",
    "        if s.notna().sum() < q * 10:\n",
    "            q = 3\n",
    "            if labels is not None:\n",
    "                labels = labels[:3]\n",
    "        try:\n",
    "            return pd.qcut(s, q=q, labels=labels, duplicates=\"drop\").astype(object)\n",
    "        except Exception:\n",
    "            return pd.Series([\"Unknown\"] * len(series), index=series.index, dtype=object)\n",
    "\n",
    "    if \"employees_total\" in df.columns:\n",
    "        df[\"log_employees\"] = np.log1p(df[\"employees_total\"])\n",
    "        df[\"size_emp_tier\"] = safe_qcut(df[\"log_employees\"], q=4, labels=[\"emp_s\", \"emp_m\", \"emp_l\", \"emp_xl\"])\n",
    "    else:\n",
    "        df[\"size_emp_tier\"] = \"Unknown\"\n",
    "\n",
    "    if \"revenue_usd\" in df.columns:\n",
    "        df[\"log_revenue\"] = np.log1p(df[\"revenue_usd\"])\n",
    "        df[\"size_rev_tier\"] = safe_qcut(df[\"log_revenue\"], q=4, labels=[\"rev_s\", \"rev_m\", \"rev_l\", \"rev_xl\"])\n",
    "    else:\n",
    "        df[\"size_rev_tier\"] = \"Unknown\"\n",
    "\n",
    "    # ---------- Corporate structure ----------\n",
    "    def to_bool_col(colname):\n",
    "        if colname not in df.columns:\n",
    "            return pd.Series([False] * len(df), index=df.index, dtype=bool)\n",
    "        s = df[colname].astype(object)\n",
    "        out = []\n",
    "        for v in s.values:\n",
    "            if pd.isna(v):\n",
    "                out.append(False)\n",
    "            else:\n",
    "                t = str(v).strip().lower()\n",
    "                out.append(True if t == \"true\" else False)\n",
    "        return pd.Series(out, index=df.index, dtype=bool)\n",
    "\n",
    "    is_hq = to_bool_col(\"is_headquarters\")\n",
    "    is_du = to_bool_col(\"is_domestic_ultimate\")\n",
    "\n",
    "    df[\"has_parent_company\"] = df[\"parent_company\"].notna() if \"parent_company\" in df.columns else False\n",
    "    df[\"has_global_ultimate\"] = df[\"global_ultimate_company\"].notna() if \"global_ultimate_company\" in df.columns else False\n",
    "    df[\"has_domestic_ultimate_company\"] = df[\"domestic_ultimate_company\"].notna() if \"domestic_ultimate_company\" in df.columns else False\n",
    "\n",
    "    if \"entity_type\" in df.columns:\n",
    "        et = df[\"entity_type\"].astype(object).apply(lambda x: str(x).lower() if not pd.isna(x) else \"\")\n",
    "    else:\n",
    "        et = pd.Series([\"\"] * len(df), index=df.index, dtype=object)\n",
    "\n",
    "    df[\"structure_tier\"] = np.select(\n",
    "        [\n",
    "            is_hq,\n",
    "            is_du,\n",
    "            et.str.contains(\"subsidi\", na=False),\n",
    "            et.str.contains(\"branch\", na=False),\n",
    "            df[\"has_parent_company\"] == True,\n",
    "            (df[\"has_global_ultimate\"] == True) | (df[\"has_domestic_ultimate_company\"] == True),\n",
    "        ],\n",
    "        [\n",
    "            \"hq\",\n",
    "            \"domestic_ultimate\",\n",
    "            \"subsidiary\",\n",
    "            \"branch\",\n",
    "            \"subsidiary_like\",\n",
    "            \"member_of_group\",\n",
    "        ],\n",
    "        default=\"standalone_like\"\n",
    "    ).astype(object)\n",
    "\n",
    "    # ---------- IT tiers ----------\n",
    "    if \"it_spend\" in df.columns:\n",
    "        df[\"log_it_spend\"] = np.log1p(df[\"it_spend\"])\n",
    "        df[\"it_spend_tier\"] = safe_qcut(df[\"log_it_spend\"], q=4, labels=[\"it_low\", \"it_mid\", \"it_high\", \"it_top\"])\n",
    "    else:\n",
    "        df[\"it_spend_tier\"] = \"Unknown\"\n",
    "\n",
    "    if \"device_total\" in df.columns:\n",
    "        df[\"log_device_total\"] = np.log1p(df[\"device_total\"])\n",
    "        df[\"device_tier\"] = safe_qcut(df[\"log_device_total\"], q=4, labels=[\"dev_low\", \"dev_mid\", \"dev_high\", \"dev_top\"])\n",
    "    else:\n",
    "        df[\"device_tier\"] = \"Unknown\"\n",
    "\n",
    "    # ---------- Geo tier ----------\n",
    "    if \"country\" in df.columns:\n",
    "        df[\"geo_tier\"] = df[\"country\"].astype(object)\n",
    "        df[\"geo_tier\"] = df[\"geo_tier\"].fillna(\"Unknown\")\n",
    "    else:\n",
    "        df[\"geo_tier\"] = \"Unknown\"\n",
    "\n",
    "    # ---------- Final label/id ----------\n",
    "    seg_parts = [\"sic_2digit\", \"size_emp_tier\", \"size_rev_tier\", \"structure_tier\", \"it_spend_tier\", \"device_tier\", \"geo_tier\"]\n",
    "    for c in seg_parts:\n",
    "        df[c] = df[c].astype(object)\n",
    "        df[c] = df[c].where(~pd.isna(df[c]), \"Unknown\")  # fill NA with \"Unknown\"\n",
    "\n",
    "    df[\"segment_label\"] = df[seg_parts].astype(str).agg(\"|\".join, axis=1)\n",
    "    df[\"segment_id\"] = df[\"segment_label\"].astype(\"category\").cat.codes\n",
    "\n",
    "    return df\n",
    "\n",
    "df = add_rule_segments(df)\n",
    "\n",
    "# -----------------------------\n",
    "# Quick data health summary\n",
    "# -----------------------------\n",
    "with st.expander(\"Data health (quick checks)\", expanded=False):\n",
    "    st.write(f\"Rows: {len(df):,}  Columns: {df.shape[1]}\")\n",
    "\n",
    "    if \"country\" in df.columns:\n",
    "        st.write(\"Country breakdown\")\n",
    "        ctab = df[\"country\"].fillna(\"Unknown\").astype(str).value_counts(dropna=False).reset_index()\n",
    "        ctab.columns = [\"country\", \"count\"]\n",
    "        st.dataframe(ctab, use_container_width=True)\n",
    "\n",
    "    key_cols = [c for c in [\n",
    "        \"sic_code\", \"8_digit_sic_code\",\n",
    "        \"employees_total\", \"revenue_usd\",\n",
    "        \"it_spend\", \"device_total\",\n",
    "        \"corporate_family_members\", \"entity_type\",\n",
    "        \"country\", \"state\", \"city\"\n",
    "    ] if c in df.columns]\n",
    "\n",
    "    if key_cols:\n",
    "        miss = (df[key_cols].isna().mean() * 100).round(1).sort_values(ascending=False)\n",
    "        miss_df = miss.reset_index()\n",
    "        miss_df.columns = [\"column\", \"missing_percent\"]\n",
    "        st.write(\"Missing % for key columns\")\n",
    "        st.dataframe(miss_df, use_container_width=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Sidebar filters\n",
    "# -----------------------------\n",
    "st.sidebar.subheader(\"Filters\")\n",
    "\n",
    "col_country = pick_col(df, [\"country\"])\n",
    "col_entity = pick_col(df, [\"entity_type\"])\n",
    "\n",
    "def multiselect_filter(label, col):\n",
    "    if col is None:\n",
    "        st.sidebar.caption(f\"⚠️ {label}: column not found\")\n",
    "        return []\n",
    "    vals = sorted([v for v in df[col].dropna().astype(str).unique()])\n",
    "    return st.sidebar.multiselect(label, vals)\n",
    "\n",
    "sel_country = multiselect_filter(\"Country\", col_country)\n",
    "sel_entity = multiselect_filter(\"Entity Type\", col_entity)\n",
    "\n",
    "seg_vals = sorted(df[\"segment_label\"].dropna().astype(str).unique())\n",
    "sel_segs = st.sidebar.multiselect(\"Segment\", seg_vals)\n",
    "\n",
    "filtered = df.copy()\n",
    "if col_country and sel_country:\n",
    "    filtered = filtered[filtered[col_country].astype(str).isin(sel_country)]\n",
    "if col_entity and sel_entity:\n",
    "    filtered = filtered[filtered[col_entity].astype(str).isin(sel_entity)]\n",
    "if sel_segs:\n",
    "    filtered = filtered[filtered[\"segment_label\"].astype(str).isin(sel_segs)]\n",
    "\n",
    "st.sidebar.caption(f\"Filtered rows: {len(filtered):,}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Tabs (basic exploration)\n",
    "# -----------------------------\n",
    "tab1, tab2 = st.tabs([\"Explore Companies\", \"Explore Segments\"])\n",
    "\n",
    "with tab1:\n",
    "    st.subheader(\"Company Explorer\")\n",
    "\n",
    "    col_name = pick_col(filtered, [\"company_name\", \"name\", \"company\"])\n",
    "    if col_name:\n",
    "        company = st.selectbox(\"Select a company\", sorted(filtered[col_name].fillna(\"UNKNOWN\").astype(str).unique()))\n",
    "        row = filtered[filtered[col_name].astype(str) == str(company)].head(1)\n",
    "        st.write(\"Company record\")\n",
    "        st.dataframe(row, use_container_width=True)\n",
    "    else:\n",
    "        st.info(\"No company name column detected (expected something like company_name). Showing table only.\")\n",
    "\n",
    "    st.subheader(\"Filtered preview (top 200 rows)\")\n",
    "    st.dataframe(filtered.head(200), use_container_width=True)\n",
    "\n",
    "with tab2:\n",
    "    st.subheader(\"Explore Segments\")\n",
    "\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        **What segmentation is doing:**  \n",
    "        We group companies into **segments** based on similarity in firmographic and operational attributes (e.g., size, geography, industry, entity type, and operational indicators).  \n",
    "        Each segment is a **peer group**—companies that “look alike” in this dataset.\n",
    "        \n",
    "        **What insights this enables:**  \n",
    "        - **Benchmarking:** compare a company against its segment’s typical profile (e.g., revenue/employee/IT spend vs peers)  \n",
    "        - **Peer discovery:** find similar companies for competitive analysis or partner targeting  \n",
    "        - **Risk/anomaly spotting:** surface companies that are unusual within their segment (e.g., high complexity for small size)\n",
    "                \"\"\"\n",
    "    )\n",
    "\n",
    "    # ---------- Helper: pick columns safely ----------\n",
    "    col_name = pick_col(filtered, [\"company_name\", \"name\", \"company\"])\n",
    "    col_sic_desc = pick_col(filtered, [\"8_digit_sic_description\", \"sic_description\"])\n",
    "    col_emp = pick_col(filtered, [\"employees_total\", \"employees_single_site\"])\n",
    "    col_rev = pick_col(filtered, [\"revenue_usd\"])\n",
    "    col_it = pick_col(filtered, [\"it_spend\", \"it_budget\"])\n",
    "    col_dev = pick_col(filtered, [\"device_total\"])\n",
    "    col_country = pick_col(filtered, [\"country\"])\n",
    "    col_entity = pick_col(filtered, [\"entity_type\"])\n",
    "\n",
    "    # ---------- Build a lightweight df just for segmentation UI (DO NOT mutate filtered) ----------\n",
    "    needed_cols = [\"segment_label\"]\n",
    "    for c in [col_name, col_sic_desc, col_emp, col_rev, col_it, col_dev, col_country, col_entity]:\n",
    "        if c and c in filtered.columns:\n",
    "            needed_cols.append(c)\n",
    "    needed_cols = list(dict.fromkeys(needed_cols))  # dedupe, keep order\n",
    "\n",
    "    seg_df_base = filtered[needed_cols].copy()\n",
    "\n",
    "    # Ensure segment_id exists WITHOUT touching filtered\n",
    "    if \"segment_id\" in filtered.columns:\n",
    "        seg_df_base[\"segment_id\"] = filtered[\"segment_id\"].values\n",
    "    else:\n",
    "        seg_df_base[\"segment_id\"] = seg_df_base[\"segment_label\"].astype(\"category\").cat.codes\n",
    "\n",
    "    # ---------- Cache heavy computations (counts + titles) ----------\n",
    "    @st.cache_data(show_spinner=False)\n",
    "    def build_segment_tables_small(df, col_country, col_entity, col_sic_desc):\n",
    "        # counts\n",
    "        seg_counts = (\n",
    "            df.groupby([\"segment_id\", \"segment_label\"], dropna=False)\n",
    "            .size()\n",
    "            .reset_index(name=\"count\")\n",
    "            .sort_values([\"count\", \"segment_id\"], ascending=[False, True])\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        seg_counts[\"segment_key\"] = seg_counts[\"segment_id\"].apply(lambda x: f\"S{int(x)}\")\n",
    "        seg_counts[\"share_%\"] = (seg_counts[\"count\"] / max(len(df), 1) * 100).round(1)\n",
    "\n",
    "        # segment title (top entity | top industry | top country) using groupby apply (cached once)\n",
    "        def top_of(col):\n",
    "            if not col or col not in df.columns:\n",
    "                return None\n",
    "            return (\n",
    "                df[[ \"segment_id\", col ]]\n",
    "                .assign(**{col: df[col].fillna(\"Unknown\").astype(str)})\n",
    "                .groupby(\"segment_id\")[col]\n",
    "                .apply(lambda s: s.value_counts().index[0] if len(s.value_counts()) else \"Unknown\")\n",
    "            )\n",
    "\n",
    "        top_country = top_of(col_country)\n",
    "        top_entity = top_of(col_entity)\n",
    "        top_ind = top_of(col_sic_desc)\n",
    "\n",
    "        # map into seg_counts by segment_id\n",
    "        seg_counts[\"top_country\"] = seg_counts[\"segment_id\"].map(top_country) if top_country is not None else \"—\"\n",
    "        seg_counts[\"top_entity\"] = seg_counts[\"segment_id\"].map(top_entity) if top_entity is not None else \"—\"\n",
    "        seg_counts[\"top_industry\"] = seg_counts[\"segment_id\"].map(top_ind) if top_ind is not None else \"—\"\n",
    "\n",
    "        seg_counts[\"segment_title\"] = (\n",
    "            seg_counts[\"top_entity\"].fillna(\"—\").astype(str)\n",
    "            + \" | \" + seg_counts[\"top_industry\"].fillna(\"—\").astype(str)\n",
    "            + \" | \" + seg_counts[\"top_country\"].fillna(\"—\").astype(str)\n",
    "        )\n",
    "\n",
    "        return seg_counts\n",
    "\n",
    "    seg_counts = build_segment_tables_small(seg_df_base, col_country, col_entity, col_sic_desc)\n",
    "\n",
    "    # ---------- Controls (form prevents slider drag reruns) ----------\n",
    "    with st.form(\"seg_controls\"):\n",
    "        st.markdown(\"### Controls (click Apply to update)\")\n",
    "\n",
    "        cL, cR = st.columns(2)\n",
    "        with cL:\n",
    "            show_n = st.slider(\"Show top N segments\", 10, min(200, len(seg_counts)), 30, 5, key=\"seg_show_n\")\n",
    "        with cR:\n",
    "            top_k = st.slider(\"Bars to plot\", 5, 30, 15, 1, key=\"seg_top_k\")\n",
    "\n",
    "        sort_mode = st.selectbox(\n",
    "            \"Sort segments by\",\n",
    "            [\"Largest segments (recommended)\", \"Segment ID\"],\n",
    "            index=0,\n",
    "            key=\"seg_sort_mode\"\n",
    "        )\n",
    "\n",
    "        applied = st.form_submit_button(\"Apply\")\n",
    "\n",
    "    # Apply sorting\n",
    "    seg_view = seg_counts.copy()\n",
    "    if sort_mode == \"Segment ID\":\n",
    "        seg_view = seg_view.sort_values([\"segment_id\"], ascending=True).reset_index(drop=True)\n",
    "    else:\n",
    "        seg_view = seg_view.sort_values([\"count\", \"segment_id\"], ascending=[False, True]).reset_index(drop=True)\n",
    "\n",
    "    # ---------- Table ----------\n",
    "    st.markdown(\"### Segment list\")\n",
    "    seg_table = seg_view.head(show_n)[[\"segment_key\", \"count\", \"share_%\", \"segment_title\", \"segment_label\"]].copy()\n",
    "    st.dataframe(seg_table, use_container_width=True, height=420)\n",
    "\n",
    "    # ---------- Chart ----------\n",
    "    st.markdown(\"### Top segments (chart)\")\n",
    "    top = seg_view.head(top_k).copy()\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    plt.barh(top[\"segment_key\"].astype(str), top[\"count\"])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"Company count\")\n",
    "    plt.ylabel(\"Segment\")\n",
    "    st.pyplot(fig, clear_figure=True)\n",
    "\n",
    "    # ---------- Segment selector (FAST: use format_func, not a giant list of strings) ----------\n",
    "    st.markdown(\"### Segment deep dive\")\n",
    "\n",
    "    # Limit dropdown to top N segments for performance\n",
    "    selector_df = seg_view.head(show_n).copy()\n",
    "\n",
    "    # Create a lookup dict for nice display in format_func\n",
    "    label_map = {}\n",
    "    for _, r in selector_df.iterrows():\n",
    "        label_map[int(r[\"segment_id\"])] = f'{r[\"segment_key\"]} — {r[\"segment_title\"]} ({int(r[\"count\"])})'\n",
    "\n",
    "    seg_ids = selector_df[\"segment_id\"].astype(int).tolist()\n",
    "\n",
    "    selected_seg_id = st.selectbox(\n",
    "        \"Select a segment (from the top list above)\",\n",
    "        options=seg_ids,\n",
    "        index=0,\n",
    "        format_func=lambda x: label_map.get(int(x), f\"S{int(x)}\"),\n",
    "        key=\"seg_selected_id\"\n",
    "    )\n",
    "\n",
    "    seg_row = seg_counts[seg_counts[\"segment_id\"] == selected_seg_id].head(1)\n",
    "    seg_label = seg_row[\"segment_label\"].iloc[0] if len(seg_row) else \"Unknown\"\n",
    "    seg_title = seg_row[\"segment_title\"].iloc[0] if len(seg_row) else \"—\"\n",
    "    seg_size = int(seg_row[\"count\"].iloc[0]) if len(seg_row) else 0\n",
    "\n",
    "    st.write(f\"**Selected:** S{int(selected_seg_id)}  |  **Companies:** {seg_size:,}\")\n",
    "    st.caption(f\"**Segment definition (auto):** {seg_title}\")\n",
    "    with st.expander(\"Raw segment label (full definition)\", expanded=False):\n",
    "        st.code(str(seg_label))\n",
    "\n",
    "    seg_df = seg_df_base[seg_df_base[\"segment_id\"] == selected_seg_id].copy()\n",
    "\n",
    "    # ---------- Snapshot ----------\n",
    "    st.markdown(\"#### Segment profile snapshot\")\n",
    "    c1, c2, c3, c4 = st.columns(4)\n",
    "    c1.metric(\"Companies\", f\"{len(seg_df):,}\")\n",
    "    c2.metric(\"Countries (unique)\", f\"{seg_df[col_country].nunique(dropna=True):,}\" if col_country else \"—\")\n",
    "    c3.metric(\"Entity types (unique)\", f\"{seg_df[col_entity].nunique(dropna=True):,}\" if col_entity else \"—\")\n",
    "    c4.metric(\"Industry labels (unique)\", f\"{seg_df[col_sic_desc].nunique(dropna=True):,}\" if col_sic_desc else \"—\")\n",
    "\n",
    "    # ---------- Benchmark ----------\n",
    "    st.markdown(\"#### Benchmark vs overall (median)\")\n",
    "    compare_cols = [c for c in [col_emp, col_rev, col_it, col_dev] if c and c in seg_df_base.columns]\n",
    "\n",
    "    def med(series):\n",
    "        x = pd.to_numeric(series, errors=\"coerce\")\n",
    "        v = np.nanmedian(x)\n",
    "        return float(v) if np.isfinite(v) else np.nan\n",
    "\n",
    "    if compare_cols:\n",
    "        bench_rows = []\n",
    "        for c in compare_cols:\n",
    "            seg_m = med(seg_df[c])\n",
    "            all_m = med(seg_df_base[c])\n",
    "            ratio = seg_m / all_m if (np.isfinite(seg_m) and np.isfinite(all_m) and all_m != 0) else np.nan\n",
    "            bench_rows.append({\n",
    "                \"metric\": c,\n",
    "                \"segment_median\": seg_m,\n",
    "                \"overall_median\": all_m,\n",
    "                \"segment_vs_overall\": round(ratio, 2) if np.isfinite(ratio) else np.nan,\n",
    "            })\n",
    "        st.dataframe(pd.DataFrame(bench_rows), use_container_width=True)\n",
    "    else:\n",
    "        st.info(\"No numeric benchmark columns found (employees/revenue/IT/device).\")\n",
    "\n",
    "    # ---------- Dominant categories ----------\n",
    "    st.markdown(\"#### What dominates this segment?\")\n",
    "    cA, cB, cC = st.columns(3)\n",
    "\n",
    "    with cA:\n",
    "        if col_country:\n",
    "            ct = seg_df[col_country].fillna(\"Unknown\").astype(str).value_counts().head(10).reset_index()\n",
    "            ct.columns = [\"country\", \"count\"]\n",
    "            st.write(\"Top countries\")\n",
    "            st.dataframe(ct, use_container_width=True, height=260)\n",
    "        else:\n",
    "            st.caption(\"Country column not found.\")\n",
    "\n",
    "    with cB:\n",
    "        if col_entity:\n",
    "            et = seg_df[col_entity].fillna(\"Unknown\").astype(str).value_counts().head(10).reset_index()\n",
    "            et.columns = [\"entity_type\", \"count\"]\n",
    "            st.write(\"Top entity types\")\n",
    "            st.dataframe(et, use_container_width=True, height=260)\n",
    "        else:\n",
    "            st.caption(\"Entity type column not found.\")\n",
    "\n",
    "    with cC:\n",
    "        if col_sic_desc:\n",
    "            ind = seg_df[col_sic_desc].fillna(\"Unknown\").astype(str).value_counts().head(10).reset_index()\n",
    "            ind.columns = [\"industry_desc\", \"count\"]\n",
    "            st.write(\"Top industries\")\n",
    "            st.dataframe(ind, use_container_width=True, height=260)\n",
    "        else:\n",
    "            st.caption(\"Industry description column not found.\")\n",
    "\n",
    "    # ---------- Sample companies ----------\n",
    "    st.markdown(\"#### Example companies in this segment\")\n",
    "    if col_name:\n",
    "        preview_cols = [c for c in [col_name, col_country, col_entity, col_emp, col_rev, col_it, col_dev]\n",
    "                        if c and c in seg_df.columns]\n",
    "        st.dataframe(seg_df[preview_cols].head(30), use_container_width=True, height=420)\n",
    "    else:\n",
    "        st.dataframe(seg_df.head(30), use_container_width=True, height=420)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# AI Assistant (Sidebar)\n",
    "# -----------------------------\n",
    "with st.sidebar:\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"🤖 AI Data Assistant\")\n",
    "    \n",
    "    # Initialize chat history\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "    # Display chat messages from history on app rerun\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    # React to user input\n",
    "    if prompt := st.chat_input(\"Ask about the data...\"):\n",
    "        # 1. Display user message\n",
    "        st.chat_message(\"user\").markdown(prompt)\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        # 2. Build Context (The filtered data from your app)\n",
    "        # Use the 'filtered' dataframe from your main script\n",
    "        data_context = get_dataframe_context(filtered, max_rows=5)\n",
    "\n",
    "        # 3. Construct the prompt for the LLM\n",
    "        full_prompt = f\"\"\"\n",
    "        You are a helpful Data Analyst assistant. \n",
    "        Analyze the following dataset snippet and answer the user's question.\n",
    "        \n",
    "        CONTEXT DATA:\n",
    "        {data_context}\n",
    "        \n",
    "        USER QUESTION: \n",
    "        {prompt}\n",
    "        \n",
    "        Answer concisely and based ONLY on the provided data context.\n",
    "        \"\"\"\n",
    "\n",
    "        # 4. Stream the response\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            try:\n",
    "                # Helper: Yields text chunks from the API response\n",
    "                def stream_generator():\n",
    "                    stream = llm_client.chat_completion(\n",
    "                        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "                        max_tokens=500,\n",
    "                        stream=True\n",
    "                    )\n",
    "                    for chunk in stream:\n",
    "                        # Extract text from the \"delta\" in the chunk\n",
    "                        if chunk.choices and chunk.choices[0].delta.content:\n",
    "                            yield chunk.choices[0].delta.content\n",
    "\n",
    "                # Streamlit writes the stream to the UI\n",
    "                response = st.write_stream(stream_generator())\n",
    "                \n",
    "                # Save the final response to history\n",
    "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "                \n",
    "            except Exception as e:\n",
    "                st.error(f\"Error communicating with API: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0f91773-c8bf-4e2d-b225-e118c6fcc497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open: http://localhost:8502  |  PID: 5248\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "p = subprocess.Popen([sys.executable, \"-m\", \"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8502\", \"--server.headless\", \"true\"])\n",
    "print(\"Open: http://localhost:8502  |  PID:\", p.pid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5383f311-af21-4ed5-83d1-2f00a16e2b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPEN THIS: http://localhost:51584 | PID: 29144\n",
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:51584\n",
      "  Network URL: http://10.249.191.123:51584\n",
      "  External URL: http://137.132.26.144:51584\n",
      "\n",
      "2026-01-21 15:30:36.681 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2026-01-21 15:30:36.718 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2026-01-21 15:30:36.762 Please replace `use_container_width` with `width`.\n",
      "\n",
      "`use_container_width` will be removed after 2025-12-31.\n",
      "\n",
      "For `use_container_width=True`, use `width='stretch'`. For `use_container_width=False`, use `width='content'`.\n",
      "2026-01-21 15:30:36.782 Serialization of dataframe to Arrow table was unsuccessful. Applying automatic fixes for column types to make the dataframe Arrow-compatible.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Yee Hui\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\streamlit\\dataframe_util.py\", line 839, in convert_pandas_df_to_arrow_bytes\n",
      "    table = pa.Table.from_pandas(df)\n",
      "  File \"pyarrow/table.pxi\", line 4796, in pyarrow.lib.Table.from_pandas\n",
      "  File \"C:\\Users\\Yee Hui\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyarrow\\pandas_compat.py\", line 651, in dataframe_to_arrays\n",
      "    arrays = [convert_column(c, f)\n",
      "              ~~~~~~~~~~~~~~^^^^^^\n"
     ]
    }
   ],
   "source": [
    "import socket, subprocess, sys, time, webbrowser\n",
    "\n",
    "def get_free_port():\n",
    "    s = socket.socket()\n",
    "    s.bind((\"\", 0))\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "port = get_free_port()\n",
    "\n",
    "p = subprocess.Popen(\n",
    "    [sys.executable, \"-m\", \"streamlit\", \"run\", \"app.py\",\n",
    "     \"--server.port\", str(port),\n",
    "     \"--server.headless\", \"true\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "time.sleep(1)\n",
    "url = f\"http://localhost:{port}\"\n",
    "print(\"OPEN THIS:\", url, \"| PID:\", p.pid)\n",
    "\n",
    "# try to open browser\n",
    "webbrowser.open(url)\n",
    "\n",
    "# print first ~30 log lines so you can see if it crashed\n",
    "for _ in range(30):\n",
    "    line = p.stdout.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    print(line, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69deaffe-6fbd-4186-9ecb-c0ec07771613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a8e57-859a-45db-87e1-347e614a31b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac2b10-d65d-4135-8016-edefdd867a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
